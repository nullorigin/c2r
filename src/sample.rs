/*!
 * Samplizer - Dynamic Pattern Generation System
 *
 * The Samplizer analyzes C and Rust file pairs to automatically generate
 * intelligent patterns for the Patternizer system. It transforms static
 * pattern matching into a dynamic, self-improving machine learning-like approach.
 *
 * Key Components:
 * - File Comparator: Parses and aligns C/Rust code segments
 * - Pattern Miner: Extracts transformation patterns from alignments
 * - Pattern Generator: Creates Patternizer-compatible pattern definitions
 * - Pattern Integrator: Adds successful patterns to permanent collection
 */

use crate::common::is_identifier;
use crate::config::Phase::Process;
use crate::error::{C2RError, Kind, Reason};
use crate::json::{Object, Value};
use crate::lock::Id;
use crate::thread::ResilientThreadPool;
use crate::PatternResult;
use crate::ReportLevel::Info;
use crate::Result;
use crate::Token;
use crate::{gen_name, Context, Entry};
use crate::{report, HandlerPattern, Phase, ReportLevel};
use core::result::Result::{Err, Ok};
use std::collections::HashMap;
use std::fs;
use std::hash::Hasher;
use std::ops::Range;
use std::path::Path;

/// Machine learning data for token analysis
#[derive(Debug, Clone)]
pub struct TokenAnalysisData {
    /// Historical success/failure patterns for tokens
    pub token_success_rates: HashMap<String, f64>,
    /// Pattern combination success rates  
    pub pattern_combinations: HashMap<String, f64>,
    /// Negative result patterns to avoid
    pub negative_patterns: HashMap<String, i32>,
    /// Positive result reinforcement
    pub positive_patterns: HashMap<String, i32>,
    /// Token sequence confidence scores
    pub sequence_scores: HashMap<String, f64>,
}
impl PartialEq for TokenAnalysisData {
    fn eq(&self, other: &Self) -> bool {
        self.token_success_rates == other.token_success_rates
            && self.pattern_combinations == other.pattern_combinations
            && self.negative_patterns == other.negative_patterns
            && self.positive_patterns == other.positive_patterns
            && self.sequence_scores == other.sequence_scores
    }
}
impl std::hash::Hash for TokenAnalysisData {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.token_success_rates.iter().for_each(|(k, v)| {
            k.hash(state);
            v.to_bits().hash(state);
        });
        self.pattern_combinations.iter().for_each(|(k, v)| {
            k.hash(state);
            v.to_bits().hash(state);
        });
        self.negative_patterns.iter().for_each(|(k, v)| {
            k.hash(state);
            v.hash(state);
        });
        self.positive_patterns.iter().for_each(|(k, v)| {
            k.hash(state);
            v.hash(state);
        });
        self.sequence_scores.iter().for_each(|(k, v)| {
            k.hash(state);
            v.to_bits().hash(state);
        });
    }
}

impl Default for TokenAnalysisData {
    fn default() -> Self {
        Self {
            token_success_rates: HashMap::new(),
            pattern_combinations: HashMap::new(),
            negative_patterns: HashMap::new(),
            positive_patterns: HashMap::new(),
            sequence_scores: HashMap::new(),
        }
    }
}

/// Confidence calculation result
#[derive(Debug, Default, Clone)]
pub struct ConfidenceResult {
    pub overall_confidence: f64,
    pub token_contribution: f64,
    pub pattern_contribution: f64,
    pub historical_contribution: f64,
    pub reasons: Vec<String>,
}

/// Integration statistics for Samplizer performance
#[derive(Debug, Default, Clone, PartialEq)]
pub struct IntegrationStats {
    pub patterns_generated: u32,
    pub patterns_validated: u32,
    pub patterns_integrated: u32,
    pub success_rate: f64,
}
impl std::hash::Hash for IntegrationStats {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.patterns_generated.hash(state);
        self.patterns_validated.hash(state);
        self.patterns_integrated.hash(state);
        self.success_rate.to_bits().hash(state);
    }
}
impl Eq for IntegrationStats {}
/// Pattern generated by Samplizer
#[derive(Debug, Clone, PartialEq)]
pub struct SamplizerPattern {
    pub pattern_id: Id,
    pub pattern_name: String,
    pub pattern_type: SegmentType,
    pub confidence_score: f64,
    pub extraction_pattern: Option<String>,
    pub c_tokens: Vec<String>,
    pub rust_tokens: Vec<String>,
}
impl Default for SamplizerPattern {
    fn default() -> Self {
        Self {
            pattern_id: Id::get("pattern"),
            pattern_name: String::new(),
            pattern_type: SegmentType::Unknown,
            confidence_score: 0.0,
            extraction_pattern: None,
            c_tokens: Vec::new(),
            rust_tokens: Vec::new(),
        }
    }
}
/// Validation result for a pattern
#[derive(Debug, Default, Clone, PartialEq)]
pub struct ValidationResult {
    pub pattern_id: Id,
    pub success_rate: f64,
    pub total_tests: u32,
    pub successful_tests: u32,
    pub details: String,
}

/// Types of code segments identified by Samplizer
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum SegmentType {
    Function,
    Struct,
    Enum,
    TypeDef,
    Variable,
    Array,
    Comment,
    Include,
    Macro,
    ControlFlow,
    Expression,
    Unknown,
}
impl Default for SegmentType {
    fn default() -> Self {
        Self::Unknown
    }
}
/// Represents a C-Rust file pair for analysis
#[derive(Debug, Default, Clone, PartialEq, Eq, Hash)]
pub struct FilePair {
    pub c_file_path: String,
    pub rust_file_path: String,
    pub c_tokens: Vec<Token>,
    pub rust_tokens: Vec<Token>,
    pub alignment_map: Vec<SegmentAlignment>,
}

/// Alignment between C and Rust code segments
#[derive(Debug, Clone, PartialEq)]
pub struct SegmentAlignment {
    pub c_segment: (usize, usize),    // Start and end token indices in C
    pub rust_segment: (usize, usize), // Start and end token indices in Rust
    pub segment_type: SegmentType,
    pub confidence: f64,
}
impl Default for SegmentAlignment {
    fn default() -> Self {
        Self {
            c_segment: (0, 0),
            rust_segment: (0, 0),
            segment_type: SegmentType::Unknown,
            confidence: 0.0,
        }
    }
}
impl std::hash::Hash for SegmentAlignment {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.c_segment.hash(state);
        self.rust_segment.hash(state);
        self.segment_type.hash(state);
        self.confidence.to_bits().hash(state);
    }
}
impl Eq for SegmentAlignment {}
/// Main Samplizer system for dynamic pattern generation
#[derive(Debug, Default, Clone)]
pub struct Samplizer {
    /// Generated patterns from analysis
    pub generated_patterns: HashMap<Id, SamplizerPattern>,
    /// File pairs that have been analyzed
    pub analyzed_pairs: Vec<FilePair>,
    /// Pattern validation results
    pub validation_results: HashMap<Id, ValidationResult>,
    /// Resilient thread pool for parallel processing
    pub thread_pool: ResilientThreadPool,
    /// Integration statistics
    pub integration_stats: IntegrationStats,
    /// Machine learning token analysis data
    pub token_analysis_data: TokenAnalysisData,
}
impl PartialEq for Samplizer {
    fn eq(&self, other: &Self) -> bool {
        self.generated_patterns == other.generated_patterns
            && self.analyzed_pairs == other.analyzed_pairs
            && self.validation_results == other.validation_results
            && self.thread_pool == other.thread_pool
            && self.integration_stats == other.integration_stats
            && self.token_analysis_data == other.token_analysis_data
    }
}
impl Eq for Samplizer {}
impl std::hash::Hash for Samplizer {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.generated_patterns
            .iter()
            .for_each(|(k, v)| k.hash(state));
        self.analyzed_pairs.iter().for_each(|v| v.hash(state));
        self.validation_results
            .iter()
            .for_each(|(k, v)| k.hash(state));
        self.thread_pool.hash(state);
        self.integration_stats.hash(state);
        self.token_analysis_data
            .token_success_rates
            .iter()
            .for_each(|(k, v)| k.hash(state));
        self.token_analysis_data
            .pattern_combinations
            .iter()
            .for_each(|(k, v)| k.hash(state));
        self.token_analysis_data
            .negative_patterns
            .iter()
            .for_each(|(k, v)| k.hash(state));
        self.token_analysis_data
            .positive_patterns
            .iter()
            .for_each(|(k, v)| k.hash(state));
        self.token_analysis_data
            .sequence_scores
            .iter()
            .for_each(|(k, v)| k.hash(state));
    }
}

impl Samplizer {
    /// Create a new Samplizer instance with resilient threading
    pub fn new() -> Self {
        let thread_pool = ResilientThreadPool::new();

        Self {
            generated_patterns: HashMap::new(),
            analyzed_pairs: Vec::new(),
            validation_results: HashMap::new(),
            thread_pool,
            integration_stats: IntegrationStats::default(),
            token_analysis_data: TokenAnalysisData::default(),
        }
    }

    /// Shutdown the samplizer's thread pool and resources
    pub fn shutdown(&mut self) {
        self.thread_pool.shutdown();
    }

    /// Generate adaptive pattern variations based on existing patterns
    pub fn generate_adaptive_variations(
        context: &mut Context,
        base_pattern: &HandlerPattern,
        deviation_bounds: f64,
    ) -> Result<Vec<SamplizerPattern>> {
        let mut variations = Vec::new();

        report!(
            context,
            "samplizer",
            "generate_adaptive_variations",
            Info,
            Process(Some("generate_adaptive_variations".to_string())),
            format!(
                "Generating adaptive variations for pattern '{}' with bounds {:.2}",
                base_pattern.id, deviation_bounds
            ),
            true
        );

        // Generate priority-based variations
        for priority_delta in [-2, -1, 1, 2] {
            let new_priority = (base_pattern.priority + priority_delta).max(0);
            let deviation = (priority_delta.abs() as f64 / 5.0).min(deviation_bounds);
            let description = base_pattern
                .description
                .clone()
                .unwrap_or("Unknown".to_string());
            if deviation <= deviation_bounds {
                let pattern_id = Id::get(&gen_name("adaptive_priority_var"));
                let variation = SamplizerPattern {
                    pattern_id: pattern_id.clone(),
                    pattern_name: format!("{} (Priority {})", description, new_priority),
                    pattern_type: SegmentType::Function,
                    confidence_score: 1.0 - deviation,
                    extraction_pattern: Some(format!("Priority variation of {}", base_pattern.id)),
                    c_tokens: Vec::new(),
                    rust_tokens: Vec::new(),
                };
                variations.push(variation);
            }
        }

        // Generate token count variations using existing min_tokens field
        let min_variations = [
            base_pattern.min_tokens.saturating_sub(1),
            base_pattern.min_tokens.saturating_sub(2),
            base_pattern.min_tokens + 1,
            base_pattern.min_tokens + 2,
        ];

        for (i, new_min) in min_variations.iter().enumerate() {
            let deviation = (i as f64 * 0.1).min(deviation_bounds);
            let description = base_pattern
                .description
                .clone()
                .unwrap_or("Unknown".to_string());
            if deviation <= deviation_bounds {
                let pattern_id = Id::get(&gen_name("adaptive_tokens_var"));
                let variation = SamplizerPattern {
                    pattern_id: pattern_id.clone(),
                    pattern_name: format!("{} (Min tokens: {})", description, new_min),
                    pattern_type: SegmentType::Function,
                    confidence_score: 1.0 - deviation,
                    extraction_pattern: Some(format!(
                        "Token count variation of {}",
                        base_pattern.id
                    )),
                    c_tokens: Vec::new(),
                    rust_tokens: Vec::new(),
                };
                variations.push(variation);
            }
        }

        report!(
            context,
            "samplizer",
            "generate_adaptive_variations",
            ReportLevel::Info,
            Process(Some("generate_adaptive_variations".to_string())),
            format!("Generated {} adaptive variations", variations.len()),
            true
        );

        Ok(variations)
    }

    /// Learn from successful adaptive matches to improve future pattern generation
    pub fn learn_from_adaptive_success(
        &mut self,
        context: &mut Context,
        successful_variation: &crate::handler::PatternNode,
        tokens: &[Token],
    ) -> Result<()> {
        let pattern_id = Id::get(&gen_name("adaptive_learned"));
        let learned_pattern = SamplizerPattern {
            pattern_id: pattern_id.clone(),
            pattern_name: format!(
                "Learned from adaptive success (dev: {:.2})",
                successful_variation.deviation_distance
            ),
            pattern_type: SegmentType::Function,
            confidence_score: successful_variation.success_rate,
            extraction_pattern: Some("Learned adaptive pattern".to_string()),
            c_tokens: tokens.iter().map(|t| t.to_string()).collect(),
            rust_tokens: Vec::new(),
        };

        self.generated_patterns
            .insert(pattern_id.clone(), learned_pattern);

        // Update token analysis data with successful pattern
        let token_sequence = tokens
            .iter()
            .map(|t| t.to_string())
            .collect::<Vec<_>>()
            .join(" ");
        self.token_analysis_data
            .positive_patterns
            .entry(token_sequence.clone())
            .and_modify(|count| *count += 1)
            .or_insert(1);

        // Update sequence scores for this token pattern
        self.token_analysis_data
            .sequence_scores
            .insert(token_sequence, successful_variation.success_rate);

        report!(
            context,
            "samplizer",
            "learn_from_adaptive_success",
            ReportLevel::Info,
            Process(Some("learn_from_adaptive_success".to_string())),
            format!(
                "Learned new pattern from adaptive success with {:.2} success rate",
                successful_variation.success_rate
            ),
            true
        );

        Ok(())
    }

    /// Generate fractal-like pattern explorations using radial approach
    pub fn generate_fractal_explorations(
        context: &mut Context,
        center_pattern: &HandlerPattern,
        exploration_radius: f64,
        depth: usize,
    ) -> Result<Vec<SamplizerPattern>> {
        let mut fractal_variations = Vec::new();

        report!(
            context,
            "samplizer",
            "generate_fractal_explorations",
            ReportLevel::Info,
            Process(Some("generate_fractal_explorations".to_string())),
            format!(
                "Starting fractal exploration with radius {:.2} and depth {}",
                exploration_radius, depth
            ),
            true
        );

        let num_radial_branches = 8;
        let angle_step = 2.0 * std::f64::consts::PI / num_radial_branches as f64;

        for branch in 0..num_radial_branches {
            let angle = branch as f64 * angle_step;

            for distance_level in 1..=depth {
                let distance = exploration_radius * (distance_level as f64 / depth as f64);
                let priority_variation = (angle.cos() * distance * 10.0) as i32;
                let token_variation = (angle.sin() * distance * 5.0) as usize;

                let new_priority = (center_pattern.priority + priority_variation).max(0);
                let new_min_tokens = center_pattern.min_tokens.saturating_add(token_variation);

                let pattern_id = Id::get(&gen_name("fractal_exploration"));
                let variation = SamplizerPattern {
                    pattern_id: pattern_id.clone(),
                    pattern_name: format!(
                        "{} (Fractal branch {}, level {})",
                        center_pattern
                            .description
                            .clone()
                            .unwrap_or("Unknown".to_string()),
                        branch,
                        distance_level
                    ),
                    pattern_type: SegmentType::Function,
                    confidence_score: (1.0 - distance).max(0.1),
                    extraction_pattern: Some(format!(
                        "Fractal exploration from {}",
                        center_pattern.id
                    )),
                    c_tokens: Vec::new(),
                    rust_tokens: Vec::new(),
                };

                fractal_variations.push(variation);

                // Update integration stats for fractal generation
                context.samplizer.integration_stats.patterns_generated += 1;
            }
        }

        report!(
            context,
            "samplizer",
            "generate_fractal_explorations",
            ReportLevel::Info,
            Process(Some("generate_fractal_explorations".to_string())),
            format!(
                "Generated {} fractal variations across {} branches and {} depth levels",
                fractal_variations.len(),
                num_radial_branches,
                depth
            ),
            true
        );

        Ok(fractal_variations)
    }

    /// Analyze a C-Rust file pair to generate patterns with optional limits
    pub fn analyze_file_pair<P: AsRef<Path>>(
        &mut self,
        context: &mut Context,
        c_file_path: P,
        rust_file_path: P,
        max_patterns: Option<usize>,
    ) -> Result<Vec<SamplizerPattern>> {
        let c_file_path = c_file_path.as_ref();
        let rust_file_path = rust_file_path.as_ref();
        let _id = Id::get("samplizer_analyze");

        report!(
            context,
            "samplizer",
            "analyze_file_pair",
            ReportLevel::Info,
            Process(Some("analyze_file_pair".to_string())),
            format!(
                "Analyzing file pair: {:?} -> {:?} (max_patterns: {:?})",
                c_file_path, rust_file_path, max_patterns
            ),
            true
        );

        // Early validation with detailed error reporting
        if !c_file_path.exists() {
            report!(
                context,
                "samplizer",
                "analyze_file_pair",
                ReportLevel::Warning,
                Process(Some("analyze_file_pair".to_string())),
                format!("C file does not exist: {:?}", c_file_path),
                false
            );
            return Ok(Vec::new());
        }

        if !rust_file_path.exists() {
            report!(
                context,
                "samplizer",
                "analyze_file_pair",
                ReportLevel::Warning,
                Process(Some("analyze_file_pair".to_string())),
                format!("Rust file does not exist: {:?}", rust_file_path),
                false
            );
            return Ok(Vec::new());
        }

        // Parse file pair with error recovery
        let mut file_pair = match Self::parse_file_pair(context, c_file_path, rust_file_path) {
            Ok(pair) => pair,
            Err(e) => {
                report!(
                    context,
                    "samplizer",
                    "analyze_file_pair",
                    ReportLevel::Error,
                    Process(Some("analyze_file_pair".to_string())),
                    format!("Failed to parse file pair: {}", e),
                    false
                );
                return Err(e);
            }
        };

        // Early exit for empty files
        if file_pair.c_tokens.is_empty() && file_pair.rust_tokens.is_empty() {
            return Ok(Vec::new());
        }

        // Optimize token processing based on limits
        let effective_max_patterns = max_patterns.unwrap_or(100);
        let max_tokens = if max_patterns.is_some() { 1000 } else { 5000 };

        // Apply intelligent token truncation
        if file_pair.c_tokens.len() > max_tokens {
            file_pair.c_tokens.truncate(max_tokens);
        }
        if file_pair.rust_tokens.len() > max_tokens {
            file_pair.rust_tokens.truncate(max_tokens);
        }

        report!(
            context,
            "samplizer",
            "analyze_file_pair",
            Info,
            Process(Some("analyze_file_pair".to_string())),
            format!(
                "Processing {} C tokens, {} Rust tokens (limit: {})",
                file_pair.c_tokens.len(),
                file_pair.rust_tokens.len(),
                max_tokens
            ),
            true
        );

        // Perform alignment with error handling
        let aligned_pair = Self::align_segments(context, file_pair)?;

        // Generate patterns with capacity pre-allocation
        let mut patterns =
            Vec::with_capacity(effective_max_patterns.min(aligned_pair.alignment_map.len()));

        // Process alignments efficiently
        for (idx, alignment) in aligned_pair.alignment_map.iter().enumerate() {
            if patterns.len() >= effective_max_patterns {
                break;
            }

            match Self::generate_samplizer_pattern(&aligned_pair, alignment) {
                Ok(pattern) => {
                    patterns.push(pattern);
                }
                Err(e) => {
                    report!(
                        context,
                        "samplizer",
                        "pattern_generation",
                        ReportLevel::Warning,
                        Process(Some("pattern_generation".to_string())),
                        format!("Failed to generate pattern at index {}: {}", idx, e),
                        false
                    );
                }
            }
        }

        // Enhanced fallback pattern generation if needed
        if patterns.is_empty()
            && !aligned_pair.c_tokens.is_empty()
            && !aligned_pair.rust_tokens.is_empty()
        {
            let fallback_count = effective_max_patterns.min(3);
            let base_name = c_file_path
                .file_stem()
                .and_then(|s| s.to_str())
                .unwrap_or("unknown");

            for i in 0..fallback_count {
                let confidence = 0.85 - (i as f64 * 0.05);
                let pattern_id = Id::get(&format!("fallback_{}_{}", base_name, i));

                patterns.push(SamplizerPattern {
                    pattern_id: pattern_id.clone(),
                    pattern_name: format!("Fallback_{}_{}", base_name, i),
                    pattern_type: match i % 3 {
                        0 => SegmentType::Function,
                        1 => SegmentType::Expression,
                        _ => SegmentType::Variable,
                    },
                    confidence_score: confidence,
                    extraction_pattern: Some(format!("FALLBACK_PATTERN_{}", i)),
                    c_tokens: aligned_pair
                        .c_tokens
                        .iter()
                        .take(3)
                        .map(|t| t.to_string())
                        .collect(),
                    rust_tokens: aligned_pair
                        .rust_tokens
                        .iter()
                        .take(3)
                        .map(|t| t.to_string())
                        .collect(),
                });
            }

            report!(
                context,
                "samplizer",
                "analyze_file_pair",
                Info,
                Process(Some("analyze_file_pair".to_string())),
                format!("Generated {} fallback patterns", patterns.len()),
                true
            );
        }

        // Store analyzed pair for future reference
        self.analyzed_pairs.push(aligned_pair);

        // Update statistics atomically
        self.integration_stats.patterns_generated += patterns.len() as u32;

        report!(
            context,
            "samplizer",
            "analyze_file_pair",
            Info,
            Process(Some("analyze_file_pair".to_string())),
            format!(
                "Successfully generated {} patterns from file pair analysis",
                patterns.len()
            ),
            true
        );

        Ok(patterns)
    }

    /// Parse and tokenize a C-Rust file pair
    fn parse_file_pair<P: AsRef<Path>>(
        context: &mut Context,
        c_file_path: P,
        rust_file_path: P,
    ) -> Result<FilePair> {
        let c_file_path = c_file_path.as_ref();
        let rust_file_path = rust_file_path.as_ref();

        // Validate both files exist before processing
        if !c_file_path.exists() {
            return Err(C2RError::new(
                Kind::Io,
                Reason::Failed("C file not found"),
                Some(format!("C file does not exist: {}", c_file_path.display())),
            ));
        }

        if !rust_file_path.exists() {
            return Err(C2RError::new(
                Kind::Io,
                Reason::Failed("Rust file not found"),
                Some(format!(
                    "Rust file does not exist: {}",
                    rust_file_path.display()
                )),
            ));
        }

        // Use the existing parse_file_to_tokens function for consistency
        let c_tokens = Self::parse_file_to_tokens(context, c_file_path)?;
        let rust_tokens = Self::parse_file_to_tokens(context, rust_file_path)?;

        Ok(FilePair {
            c_file_path: c_file_path.to_string_lossy().to_string(),
            rust_file_path: rust_file_path.to_string_lossy().to_string(),
            c_tokens,
            rust_tokens,
            alignment_map: Vec::new(),
        })
    }

    /// Align code segments between C and Rust files
    fn align_segments(context: &mut Context, mut file_pair: FilePair) -> Result<FilePair> {
        report!(
            context,
            "samplizer",
            "align_segments",
            ReportLevel::Info,
            Process(Some("align_segments".to_string())),
            "Aligning C and Rust code segments",
            true
        );

        let c_tokens = &file_pair.c_tokens;
        let rust_tokens = &file_pair.rust_tokens;

        // Enhanced minimum token validation
        if c_tokens.len() < 4 || rust_tokens.len() < 4 {
            file_pair.alignment_map = Vec::new();
            return Ok(file_pair);
        }

        // Adaptive window sizes based on token count
        let base_window_sizes = vec![3, 5, 8];
        let window_sizes: Vec<usize> = base_window_sizes
            .into_iter()
            .filter(|&size| c_tokens.len() >= size && rust_tokens.len() >= size)
            .collect();

        if window_sizes.is_empty() {
            file_pair.alignment_map = Vec::new();
            return Ok(file_pair);
        }

        // Dynamic window limit based on max_alignments
        let max_alignments = 1000;
        let max_windows_per_size = (max_alignments * 2).min(150).max(50);

        // Process with capacity pre-allocation
        let mut alignment_results: Vec<Vec<SegmentAlignment>> =
            Vec::with_capacity(window_sizes.len());

        for window_size in window_sizes {
            let result = Self::process_window_size_limited(
                c_tokens,
                rust_tokens,
                window_size,
                max_windows_per_size,
            );
            if !result.is_empty() {
                alignment_results.push(result);
            }
        }

        // Efficient flattening and sorting
        let mut alignments: Vec<SegmentAlignment> =
            alignment_results.into_iter().flatten().collect();

        // Multi-criteria sorting for better quality selection
        alignments.sort_by(|a, b| {
            // Primary: confidence score
            match b
                .confidence
                .partial_cmp(&a.confidence)
                .unwrap_or(std::cmp::Ordering::Equal)
            {
                std::cmp::Ordering::Equal => {
                    // Secondary: segment type priority
                    let a_priority = match a.segment_type {
                        SegmentType::Function => 3,
                        SegmentType::Expression => 2,
                        _ => 1,
                    };
                    let b_priority = match b.segment_type {
                        SegmentType::Function => 3,
                        SegmentType::Expression => 2,
                        _ => 1,
                    };
                    b_priority.cmp(&a_priority)
                }
                other => other,
            }
        });

        // Sophisticated deduplication with segment type consideration
        alignments.dedup_by(|a, b| {
            let position_match = a.c_segment.0 == b.c_segment.0;
            let size_similar =
                (a.c_segment.1 - a.c_segment.0).abs_diff(b.c_segment.1 - b.c_segment.0) <= 1;
            let type_match = a.segment_type == b.segment_type;

            position_match && size_similar && type_match
        });

        // Ensure we don't exceed the requested limit
        alignments.truncate(max_alignments);

        file_pair.alignment_map = alignments;
        Ok(file_pair)
    }
    /// Find high-quality alignments using semantic analysis with parallel processing
    fn find_high_quality_alignments(
        c_tokens: &[Token],
        rust_tokens: &[Token],
    ) -> Result<Vec<SegmentAlignment>> {
        let quality_threshold = 0.80; // Increased threshold for better quality patterns

        // Optimized window sizes based on common code patterns
        let window_sizes = [4, 6, 10, 15, 25]; // More balanced sizes for better coverage

        // Pre-allocate with estimated capacity
        let mut alignment_results: Vec<Vec<SegmentAlignment>> =
            Vec::with_capacity(window_sizes.len());

        // Process window sizes with early termination for performance
        for &window_size in &window_sizes {
            // Skip if tokens are too small for this window
            if c_tokens.len() < window_size || rust_tokens.len() < window_size {
                continue;
            }

            let result =
                Self::process_window_size(c_tokens, rust_tokens, window_size, quality_threshold);
            if !result.is_empty() {
                alignment_results.push(result);
            }
        }

        // Flatten with known capacity for better memory efficiency
        let total_capacity: usize = alignment_results.iter().map(|v| v.len()).sum();
        let mut alignments: Vec<SegmentAlignment> = Vec::with_capacity(total_capacity);

        for result in alignment_results {
            alignments.extend(result);
        }

        // Enhanced deduplication with overlap detection
        alignments.sort_by(|a, b| {
            // Primary sort by confidence, secondary by segment length
            match b
                .confidence
                .partial_cmp(&a.confidence)
                .unwrap_or(std::cmp::Ordering::Equal)
            {
                std::cmp::Ordering::Equal => {
                    let a_len = a.c_segment.1 - a.c_segment.0;
                    let b_len = b.c_segment.1 - b.c_segment.0;
                    b_len.cmp(&a_len)
                }
                other => other,
            }
        });

        // Improved deduplication considering segment overlap
        alignments.dedup_by(|a, b| {
            let overlap_threshold = 0.7;
            let a_range = a.c_segment.0..a.c_segment.1;
            let b_range = b.c_segment.0..b.c_segment.1;

            let overlap_start = a_range.start.max(b_range.start);
            let overlap_end = a_range.end.min(b_range.end);

            if overlap_end > overlap_start {
                let overlap_size = overlap_end - overlap_start;
                let min_segment_size = (a_range.len()).min(b_range.len());
                let overlap_ratio = overlap_size as f64 / min_segment_size as f64;
                overlap_ratio >= overlap_threshold
            } else {
                false
            }
        });

        // Dynamic truncation based on quality distribution
        let high_quality_count = alignments
            .iter()
            .take(1500)
            .filter(|a| a.confidence > 0.85)
            .count();
        let truncate_limit = if high_quality_count > 500 { 800 } else { 1200 };
        alignments.truncate(truncate_limit);

        Ok(alignments)
    }

    /// Process a specific window size with strict limits
    fn process_window_size_limited(
        c_tokens: &[Token],
        rust_tokens: &[Token],
        window_size: usize,
        max_windows: usize,
    ) -> Vec<SegmentAlignment> {
        let mut window_alignments = Vec::new();
        let quality_threshold = 0.65; // Slightly lower threshold for more pattern discovery

        let max_c_windows = (c_tokens.len().saturating_sub(window_size)).min(max_windows);

        for i in 0..max_c_windows {
            if i >= max_windows {
                break;
            } // Extra safety check

            let c_window = &c_tokens[i..i + window_size];

            // Find best match in Rust tokens (limited search)
            let max_rust_windows = (rust_tokens.len().saturating_sub(window_size)).min(max_windows);
            let best_match = (0..max_rust_windows)
                .map(|j| {
                    let rust_window = &rust_tokens[j..j + window_size];
                    let similarity = Self::calculate_similarity(c_window, rust_window);
                    (j, rust_window, similarity)
                })
                .max_by(|a, b| a.2.partial_cmp(&b.2).unwrap_or(std::cmp::Ordering::Equal));

            if let Some((j, rust_window, similarity)) = best_match {
                if similarity > quality_threshold {
                    let segment_type = Self::classify_segment_type(c_window);

                    // Only include high-quality, semantically meaningful patterns
                    if Self::is_pattern_meaningful(c_window, rust_window, &segment_type) {
                        window_alignments.push(SegmentAlignment {
                            c_segment: (i, i + window_size),
                            rust_segment: (j, j + window_size),
                            confidence: similarity,
                            segment_type,
                        });
                    }
                }
            }
        }

        window_alignments
    }

    /// Parse a file to tokens with enhanced error handling and validation
    fn parse_file_to_tokens<P: AsRef<Path>>(
        context: &mut Context,
        file_path: P,
    ) -> Result<Vec<Token>> {
        let file_path = file_path.as_ref();

        // Validate file exists and is readable
        if !file_path.exists() {
            return Err(C2RError::new(
                Kind::Io,
                Reason::Failed("file not found"),
                Some(format!("File does not exist: {}", file_path.display())),
            ));
        }

        let content = fs::read_to_string(file_path).map_err(|e| {
            C2RError::new(
                Kind::Io,
                Reason::Failed("to read file"),
                Some(format!(
                    "Failed to read file {}: {}",
                    file_path.display(),
                    e
                )),
            )
        })?;

        // Validate content is not empty
        if content.is_empty() {
            return Ok(Vec::new());
        }

        // Use the context tokenizer with proper error handling
        let tokens = context
            .tokenizer
            .tokenize(content.into_bytes())
            .map_err(|e| {
                C2RError::new(
                    Kind::Io,
                    Reason::Failed("to tokenize file content"),
                    Some(format!(
                        "Tokenization failed for {}: {}",
                        file_path.display(),
                        e
                    )),
                )
            })?;

        Ok(tokens)
    }

    /// Generate a Samplizer pattern from alignment (alias for extract_pattern_from_alignment)
    fn generate_samplizer_pattern(
        file_pair: &FilePair,
        alignment: &SegmentAlignment,
    ) -> Result<SamplizerPattern> {
        Self::extract_pattern_from_alignment(file_pair, alignment)
    }

    /// Process a specific window size.
    fn process_window_size(
        c_tokens: &[Token],
        rust_tokens: &[Token],
        window_size: usize,
        quality_threshold: f64,
    ) -> Vec<SegmentAlignment> {
        let window_count = c_tokens.len().saturating_sub(window_size - 1);
        if window_count == 0 {
            return Vec::new();
        }

        (0..window_count)
            .into_iter()
            .filter_map(|i| {
                let c_window = &c_tokens[i..i + window_size];

                // Early exit for low-quality windows
                if !Self::is_window_meaningful(c_window) {
                    return None;
                }

                // Find best matching Rust window with optimized search
                let rust_window_count = rust_tokens.len().saturating_sub(window_size - 1);
                let best_match = (0..rust_window_count)
                    .into_iter()
                    .map(|j| {
                        let rust_window = &rust_tokens[j..j + window_size];
                        let similarity = Self::calculate_similarity(c_window, rust_window);
                        (j, similarity)
                    })
                    .max_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal))?;

                let (j, similarity) = best_match;
                if similarity <= quality_threshold {
                    return None;
                }

                let rust_window = &rust_tokens[j..j + window_size];
                let segment_type = Self::classify_segment_type(c_window);

                // Final quality check for semantic meaningfulness
                if Self::is_pattern_meaningful(c_window, rust_window, &segment_type) {
                    Some(SegmentAlignment {
                        c_segment: (i, i + window_size),
                        rust_segment: (j, j + window_size),
                        confidence: similarity,
                        segment_type,
                    })
                } else {
                    None
                }
            })
            .collect()
    }

    /// Check if a token window contains meaningful content worth analyzing
    fn is_window_meaningful(tokens: &[Token]) -> bool {
        if tokens.len() < 2 {
            return false;
        }

        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();

        // Count different types of tokens
        let simple_punctuation = token_strings
            .iter()
            .filter(|t| matches!(t.as_str(), ";" | "," | "(" | ")" | "{" | "}" | "[" | "]"))
            .count();

        let whitespace_tokens = token_strings
            .iter()
            .filter(|t| matches!(t.as_str(), " " | "\t" | "\n" | "\r"))
            .count();

        let meaningful_keywords = token_strings
            .iter()
            .filter(|t| {
                matches!(
                    t.as_str(),
                    "struct"
                        | "enum"
                        | "typedef"
                        | "int"
                        | "char"
                        | "float"
                        | "void"
                        | "if"
                        | "while"
                        | "for"
                        | "return"
                        | "static"
                        | "const"
                        | "unsigned"
                        | "signed"
                        | "long"
                        | "short"
                        | "double"
                        | "extern"
                        | "inline"
                )
            })
            .count();

        let identifiers = token_strings
            .iter()
            .filter(|t| {
                let s = t.as_str();
                !s.is_empty()
                    && s.chars().next().unwrap().is_alphabetic()
                    && s.chars().all(|c| c.is_alphanumeric() || c == '_')
                    && !matches!(
                        s,
                        "struct"
                            | "enum"
                            | "typedef"
                            | "int"
                            | "char"
                            | "float"
                            | "void"
                            | "if"
                            | "while"
                            | "for"
                            | "return"
                            | "static"
                            | "const"
                            | "unsigned"
                            | "signed"
                            | "long"
                            | "short"
                            | "double"
                            | "extern"
                            | "inline"
                    )
            })
            .count();

        // Window is meaningful if it has:
        // 1. At least one meaningful keyword OR identifier
        // 2. Not dominated by punctuation (< 70% of tokens)
        // 3. Reasonable content-to-noise ratio
        let content_tokens = meaningful_keywords + identifiers;
        let noise_tokens = simple_punctuation + whitespace_tokens;

        content_tokens > 0
            && simple_punctuation < (tokens.len() * 7 / 10)
            && content_tokens >= noise_tokens / 3
    }
    /// Calculate similarity between token sequences with enhanced semantic analysis
    fn calculate_similarity(c_tokens: &[Token], rust_tokens: &[Token]) -> f64 {
        let c_strings: Vec<String> = c_tokens.iter().map(|t| t.to_string()).collect();
        let rust_strings: Vec<String> = rust_tokens.iter().map(|t| t.to_string()).collect();

        let mut similarity_score = 0.0;
        let mut total_weight = 0.0;

        // 1. Direct token matching - Weight: 2.0
        let common_count = c_strings
            .iter()
            .filter(|c_token| rust_strings.contains(c_token))
            .count();
        let direct_similarity =
            common_count as f64 / c_strings.len().max(rust_strings.len()) as f64;
        similarity_score += direct_similarity * 2.0;
        total_weight += 2.0;

        // 2. Semantic type mapping - Weight: 4.0
        let semantic_similarity = Self::calculate_semantic_similarity(&c_strings, &rust_strings);
        similarity_score += semantic_similarity * 4.0;
        total_weight += 4.0;

        // 3. Structural similarity - Weight: 3.0
        let structural_similarity =
            Self::calculate_structural_similarity(&c_strings, &rust_strings);
        similarity_score += structural_similarity * 3.0;
        total_weight += 3.0;

        // 4. Length compatibility - Weight: 1.0
        let length_similarity = 1.0
            - (c_tokens.len() as f64 - rust_tokens.len() as f64).abs()
            / c_tokens.len().max(rust_tokens.len()) as f64;
        similarity_score += length_similarity * 1.0;
        total_weight += 1.0;

        // 5. Pattern complexity bonus - Weight: 2.0
        let complexity_bonus = Self::calculate_pattern_complexity_from_comparison(&c_strings, &rust_strings);
        similarity_score += complexity_bonus * 2.0;
        total_weight += 2.0;

        similarity_score / total_weight
    }

    /// Calculate structural similarity (brackets, parentheses, etc.)
    fn calculate_structural_similarity(c_strings: &[String], rust_strings: &[String]) -> f64 {
        let structural_tokens = vec![
            "(", ")", "{", "}", "[", "]", ";", ",", "=", "->", ".", "::", ":", "&", "*", "+", "-",
            "/", "%", "!", "<", ">", "<=", ">=", "==", "!=", "&&", "||", "|", "^", "<<", ">>",
            "++", "--", "+=", "-=", "*=", "/=", "%=", "&=", "|=", "^=", "<<=", ">>=", "?", "~",
            "#", "::",
        ];

        let c_structural: Vec<&String> = c_strings
            .iter()
            .filter(|s| structural_tokens.contains(&s.as_str()))
            .collect();
        let rust_structural: Vec<&String> = rust_strings
            .iter()
            .filter(|s| structural_tokens.contains(&s.as_str()))
            .collect();

        if c_structural.is_empty() && rust_structural.is_empty() {
            return 1.0;
        }

        let matches = c_structural
            .iter()
            .zip(rust_structural.iter())
            .filter(|(a, b)| a == b)
            .count();
        matches as f64 / c_structural.len().max(rust_structural.len()) as f64
    }

    /// Calculate semantic similarity (types, keywords)
    fn calculate_semantic_similarity(c_strings: &[String], rust_strings: &[String]) -> f64 {
        // C to Rust type mappings
        let type_mappings = vec![
            ("int", "i32"),
            ("char", "i8"),
            ("float", "f32"),
            ("double", "f64"),
            ("void", "()"),
            ("unsigned int", "u32"),
            ("unsigned char", "u8"),
            ("long", "i64"),
            ("short", "i16"),
            ("bool", "bool"),
            ("_Bool", "bool"),
            ("unsigned long", "u64"),
            ("unsigned short", "u16"),
            ("signed", "i32"),
            ("unsigned", "u32"),
            ("size_t", "usize"),
            ("ssize_t", "isize"),
            ("ptrdiff_t", "isize"),
            ("intptr_t", "isize"),
            ("uintptr_t", "usize"),
            ("int8_t", "i8"),
            ("int16_t", "i16"),
            ("int32_t", "i32"),
            ("int64_t", "i64"),
            ("uint8_t", "u8"),
            ("uint16_t", "u16"),
            ("uint32_t", "u32"),
            ("uint64_t", "u64"),
            ("intmax_t", "i64"),
            ("uintmax_t", "u64"),
            ("int_fast8_t", "i8"),
            ("int_fast16_t", "i16"),
            ("int_fast32_t", "i32"),
            ("int_fast64_t", "i64"),
            ("uint_fast8_t", "u8"),
            ("uint_fast16_t", "u16"),
            ("uint_fast32_t", "u32"),
            ("uint_fast64_t", "u64"),
            ("int_least8_t", "i8"),
            ("int_least16_t", "i16"),
            ("int_least32_t", "i32"),
            ("int_least64_t", "i64"),
            ("uint_least8_t", "u8"),
            ("uint_least16_t", "u16"),
            ("uint_least32_t", "u32"),
            ("uint_least64_t", "u64"),
        ];

        let mut semantic_score = 0.0;
        let mut semantic_count = 0;

        for c_token in c_strings {
            if let Some((_, rust_equiv)) = type_mappings
                .iter()
                .find(|(c_type, _)| c_type == &c_token.as_str())
            {
                if rust_strings.contains(&rust_equiv.to_string()) {
                    semantic_score += 1.0;
                }
                semantic_count += 1;
            }
        }

        if semantic_count == 0 {
            // Check for direct keyword matches
            let keywords = vec!["struct", "enum", "typedef", "static", "const"];
            let common_keywords = c_strings
                .iter()
                .filter(|s| keywords.contains(&s.as_str()) && rust_strings.contains(s))
                .count();
            return common_keywords as f64 / c_strings.len().max(1) as f64;
        }

        semantic_score / semantic_count as f64
    }

    /// Calculate pattern complexity bonus
    fn calculate_pattern_complexity_from_comparison(c_strings: &[String], rust_strings: &[String]) -> f64 {
        // Reward patterns with meaningful structure
        let complexity_indicators = vec![
            "struct", "enum", "typedef", "(", ")", "{", "}", "[", "]", "for", "while", "if",
        ];

        let c_complexity = c_strings
            .iter()
            .filter(|s| complexity_indicators.contains(&s.as_str()))
            .count();
        let rust_complexity = rust_strings
            .iter()
            .filter(|s| complexity_indicators.contains(&s.as_str()))
            .count();

        let avg_complexity = (c_complexity + rust_complexity) as f64 / 2.0;
        let max_len = c_strings.len().max(rust_strings.len()) as f64;

        (avg_complexity / max_len).min(1.0)
    }

    /// Enhanced segment type classification with context awareness
    fn classify_segment_type(tokens: &[Token]) -> SegmentType {
        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();

        // Priority-based classification for better accuracy

        // 1. Preprocessor directives
        if token_strings.iter().any(|s| s.starts_with('#')) {
            return SegmentType::Include;
        }

        // 2. Typedef patterns
        if token_strings.contains(&"typedef".to_string()) {
            if token_strings.contains(&"struct".to_string()) {
                return SegmentType::Struct; // typedef struct
            } else if token_strings.contains(&"enum".to_string()) {
                return SegmentType::Enum; // typedef enum  
            }
            return SegmentType::TypeDef;
        }

        // 3. Struct patterns
        if token_strings.contains(&"struct".to_string()) {
            return SegmentType::Struct;
        }

        // 4. Enum patterns
        if token_strings.contains(&"enum".to_string()) {
            return SegmentType::Enum;
        }

        // 5. Array patterns (type + identifier + brackets)
        if let Some(bracket_pos) = token_strings.iter().position(|s| s == "[") {
            if bracket_pos > 1 && Self::is_type_keyword(&token_strings[bracket_pos - 2]) {
                return SegmentType::Array;
            }
        }

        // 6. Function patterns (more sophisticated detection)
        if let Some(paren_pos) = token_strings.iter().position(|s| s == "(") {
            if paren_pos > 0 {
                // Check if preceded by identifier and optionally by type
                let has_identifier =
                    paren_pos >= 1 && Self::is_identifier(&token_strings[paren_pos - 1]);
                let has_type =
                    paren_pos >= 2 && Self::is_type_keyword(&token_strings[paren_pos - 2]);

                if has_identifier && (has_type || paren_pos == 1) {
                    return SegmentType::Function;
                }
            }
        }

        // 7. Control flow patterns
        let control_keywords = vec!["if", "while", "for", "switch", "do"];
        if token_strings
            .iter()
            .any(|s| control_keywords.contains(&s.as_str()))
        {
            return SegmentType::ControlFlow;
        }

        // 8. Variable declarations
        if token_strings.len() >= 2
            && Self::is_type_keyword(&token_strings[0])
            && Self::is_identifier(&token_strings[1])
        {
            return SegmentType::Variable;
        }

        SegmentType::Expression
    }
    /// Check if token is a C type keyword
    fn is_type_keyword(token_string: &str) -> bool {
        matches!(
            token_string,
            "int"
                | "char"
                | "float"
                | "double"
                | "void"
                | "long"
                | "short"
                | "unsigned"
                | "signed"
                | "bool"
                | "_Bool"
                | "size_t"
                | "ssize_t"
                | "ptrdiff_t"
                | "intptr_t"
                | "uintptr_t"
                | "int8_t"
                | "int16_t"
                | "int32_t"
                | "int64_t"
                | "uint8_t"
                | "uint16_t"
                | "uint32_t"
                | "uint64_t"
                | "intmax_t"
                | "uintmax_t"
                | "int_fast8_t"
                | "int_fast16_t"
                | "int_fast32_t"
                | "int_fast64_t"
                | "uint_fast8_t"
                | "uint_fast16_t"
                | "uint_fast32_t"
                | "uint_fast64_t"
                | "int_least8_t"
                | "int_least16_t"
                | "int_least32_t"
                | "int_least64_t"
                | "uint_least8_t"
                | "uint_least16_t"
                | "uint_least32_t"
                | "uint_least64_t"
        )
    }

    /// Check if token is a valid identifier
    fn is_identifier(token: &str) -> bool {
        !token.is_empty()
            && (token.chars().next().unwrap().is_alphabetic() || token.starts_with('_'))
            && token.chars().all(|c| c.is_alphanumeric() || c == '_')
    }

    /// Check if pattern is semantically meaningful and worth including
    fn is_pattern_meaningful(
        c_tokens: &[Token],
        rust_tokens: &[Token],
        segment_type: &SegmentType,
    ) -> bool {
        let c_strings: Vec<String> = c_tokens.iter().map(|t| t.to_string()).collect();
        let rust_strings: Vec<String> = rust_tokens.iter().map(|t| t.to_string()).collect();

        // Reject patterns that are too generic or meaningless

        // 1. Must have minimum complexity
        if c_tokens.len() < 3 {
            return false;
        }

        // 2. Reject patterns with too many consecutive punctuation
        let consecutive_punct = c_strings.windows(3).any(|window| {
            window
                .iter()
                .all(|s| s.len() == 1 && !s.chars().next().unwrap().is_alphanumeric())
        });
        if consecutive_punct {
            return false;
        }

        // 3. Must contain at least one meaningful token
        let meaningful_tokens = vec![
            "struct", "enum", "typedef", "int", "char", "float", "void", "if", "while", "for",
        ];
        let has_meaningful = c_strings
            .iter()
            .any(|s| meaningful_tokens.contains(&s.as_str()));
        if !has_meaningful {
            return false;
        }

        // 4. Type-specific validation
        match segment_type {
            SegmentType::Function => {
                // Functions must have parentheses and reasonable structure
                c_strings.contains(&"(".to_string()) && c_strings.contains(&")".to_string())
            }
            SegmentType::Array => {
                // Arrays must have brackets
                c_strings.contains(&"[".to_string()) && c_strings.contains(&"]".to_string())
            }
            SegmentType::Struct | SegmentType::Enum => {
                // Structures must have meaningful identifiers
                c_strings.len() >= 2
            }
            _ => true,
        }
    }

    /// Mine patterns from aligned segments
    fn mine_patterns(&mut self, file_pair: &FilePair) -> Result<Vec<SamplizerPattern>> {
        let mut patterns = Vec::new();

        for alignment in &file_pair.alignment_map {
            let pattern = Self::extract_pattern_from_alignment(file_pair, alignment)?;

            // Store the pattern
            self.generated_patterns
                .insert(pattern.pattern_id.clone(), pattern.clone());
            patterns.push(pattern);
        }

        Ok(patterns)
    }

    /// Extract a pattern from a specific alignment
    fn extract_pattern_from_alignment(
        file_pair: &FilePair,
        alignment: &SegmentAlignment,
    ) -> Result<SamplizerPattern> {
        let pattern_id = Id::get(&gen_name("samplizer_pattern"));
        let pattern_name = format!(
            "{:?}_pattern_{}",
            alignment.segment_type,
            &pattern_id.name()[pattern_id.name().len() - 8..]
        );

        // Extract C tokens for extraction pattern
        let c_segment = &file_pair.c_tokens[alignment.c_segment.0..alignment.c_segment.1];
        let extraction_pattern = Self::generate_extraction_pattern(c_segment);

        // Extract transformation pattern from C->Rust differences
        let rust_segment =
            &file_pair.rust_tokens[alignment.rust_segment.0..alignment.rust_segment.1];

        Ok(SamplizerPattern {
            pattern_id,
            pattern_name,
            pattern_type: alignment.segment_type.clone(),
            confidence_score: alignment.confidence,
            extraction_pattern: Some(extraction_pattern),
            c_tokens: c_segment.iter().map(|t| t.to_string()).collect(),
            rust_tokens: rust_segment.iter().map(|t| t.to_string()).collect(),
        })
    }

    /// Generate high-quality, semantic extraction pattern (what to look for in C code)
    fn generate_extraction_pattern(tokens: &[Token]) -> String {
        let mut semantic_pattern = Vec::new();
        let _token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();

        for (i, token) in tokens.iter().enumerate() {
            let token_str = token.to_string();
            let pattern = match token_str.as_str() {
                // C type keywords - semantic meaning preserved
                "int" | "char" | "float" | "double" | "void" | "long" | "short" | "unsigned"
                | "signed" => "TYPE_KEYWORD",

                // Storage class specifiers
                "static" | "extern" | "inline" | "const" | "volatile" | "auto" | "register" => {
                    "STORAGE_CLASS"
                }

                // Structure keywords
                "struct" | "union" | "enum" | "typedef" => "STRUCTURE_KEYWORD",

                // Control flow keywords
                "if" | "else" | "while" | "for" | "do" | "switch" | "case" | "default"
                | "break" | "continue" | "return" | "goto" => "CONTROL_KEYWORD",

                // Function-like patterns (identifier followed by parentheses)
                s if s.chars().all(|c| c.is_alphabetic() || c == '_')
                    && i + 1 < tokens.len()
                    && tokens[i + 1].to_string() == "(" =>
                    {
                        "FUNCTION_NAME"
                    }

                // Array patterns (identifier followed by brackets)
                s if s.chars().all(|c| c.is_alphabetic() || c == '_')
                    && i + 1 < tokens.len()
                    && tokens[i + 1].to_string() == "[" =>
                    {
                        "ARRAY_NAME"
                    }

                // Variable/identifier patterns
                s if s.chars().all(|c| c.is_alphabetic() || c == '_') && s.len() > 1 => {
                    // Context-aware identifier classification
                    if i > 0 {
                        match tokens[i - 1].to_string().as_str() {
                            "struct" | "enum" | "union" => "TYPE_IDENTIFIER",
                            "int" | "char" | "float" | "double" | "void" => "VARIABLE_NAME",
                            _ => "IDENTIFIER",
                        }
                    } else {
                        "IDENTIFIER"
                    }
                }

                // Numeric literals with context
                s if s.chars().all(|c| c.is_numeric()) => {
                    if i > 0 && tokens[i - 1].to_string() == "[" {
                        "ARRAY_SIZE"
                    } else {
                        "NUMBER"
                    }
                }

                // Preserve important operators and punctuation
                "(" | ")" | "{" | "}" | "[" | "]" | ";" | "," | "=" | "+" | "-" | "*" | "/"
                | "%" => &token_str,

                // Handle preprocessor directives
                s if s.starts_with('#') => "PREPROCESSOR",

                // Everything else as literal
                _ => &token_str,
            };
            semantic_pattern.push(pattern.to_string());
        }

        semantic_pattern.join(" ")
    }

    /// Generate conversion pattern from C->Rust transformation
    fn generate_conversion_pattern(c_tokens: &[Token], rust_tokens: &[Token]) -> String {
        // Analyze the transformation from C to Rust
        let c_str = c_tokens
            .iter()
            .map(|t| t.to_string())
            .collect::<Vec<_>>()
            .join(" ");
        let rust_str = rust_tokens
            .iter()
            .map(|t| t.to_string())
            .collect::<Vec<_>>()
            .join(" ");

        format!("C: {} -> Rust: {}", c_str, rust_str)
    }

    /// Validate generated patterns against test cases
    pub fn validate_patterns(
        &mut self,
        context: &mut Context,
        test_cases: &[FilePair],
    ) -> Result<()> {
        report!(
            context,
            "samplizer",
            "validate_patterns",
            ReportLevel::Info,
            Phase::Process(Some("validate_patterns".to_string())),
            format!(
                "Validating {} patterns against {} test cases",
                self.generated_patterns.len(),
                test_cases.len()
            ),
            true,
            0,
            0
        );

        for (pattern_id, pattern) in self.generated_patterns.iter() {
            let result = self.validate_pattern(pattern, test_cases)?;
            self.validation_results.insert(pattern_id.clone(), result);
        }

        self.integration_stats.patterns_validated = self.validation_results.len() as u32;
        Ok(())
    }

    /// Validate a single pattern against test cases
    fn validate_pattern(
        &self,
        pattern: &SamplizerPattern,
        test_cases: &[FilePair],
    ) -> Result<ValidationResult> {
        let mut successful_tests = 0;
        let test_count = test_cases.len() as u32;
        let mut failure_reasons = Vec::new();

        // Test the pattern against each test case
        for test_case in test_cases {
            if Samplizer::test_pattern_on_file_pair(pattern, test_case)? {
                successful_tests += 1;
            } else {
                failure_reasons.push("Pattern did not match expected segments".to_string());
            }
        }

        let success_rate = successful_tests as f64 / test_count as f64;

        Ok(ValidationResult {
            pattern_id: pattern.pattern_id.clone(),
            success_rate,
            total_tests: test_count,
            successful_tests: successful_tests as u32,
            details: failure_reasons.join("; "),
        })
    }

    /// Test a pattern against a specific file pair
    fn test_pattern_on_file_pair(pattern: &SamplizerPattern, file_pair: &FilePair) -> Result<bool> {
        // Test if the pattern can successfully match segments in the file pair
        for alignment in &file_pair.alignment_map {
            // Check if pattern type matches alignment type
            if pattern.pattern_type == alignment.segment_type {
                // Extract tokens from the alignment segments
                let c_segment = &file_pair.c_tokens[alignment.c_segment.0..alignment.c_segment.1];
                let rust_segment =
                    &file_pair.rust_tokens[alignment.rust_segment.0..alignment.rust_segment.1];

                // Convert tokens to strings for comparison
                let c_tokens_str: Vec<String> = c_segment.iter().map(|t| t.to_string()).collect();
                let rust_tokens_str: Vec<String> =
                    rust_segment.iter().map(|t| t.to_string()).collect();

                // Check if pattern tokens match the alignment tokens
                let c_match = Samplizer::tokens_match(&pattern.c_tokens, &c_tokens_str);
                let rust_match = Samplizer::tokens_match(&pattern.rust_tokens, &rust_tokens_str);

                // Pattern succeeds if it matches both C and Rust segments with high confidence
                if c_match && rust_match && pattern.confidence_score > 0.7 {
                    return Ok(true);
                }
            }
        }

        // Pattern failed to match any alignment in the file pair
        Ok(false)
    }

    /// Helper function to check if pattern tokens match segment tokens
    fn tokens_match(pattern_tokens: &[String], segment_tokens: &[String]) -> bool {
        if pattern_tokens.len() != segment_tokens.len() {
            return false;
        }

        // Allow for some flexibility in token matching
        let matches = pattern_tokens
            .iter()
            .zip(segment_tokens.iter())
            .filter(|(p, s)| *p == *s || Samplizer::tokens_are_similar(p, s))
            .count();

        // Require at least 80% token similarity
        (matches as f64 / pattern_tokens.len() as f64) >= 0.8
    }

    /// Check if two tokens are similar enough to be considered a match
    fn tokens_are_similar(token1: &str, token2: &str) -> bool {
        // Handle common variations
        token1.to_lowercase() == token2.to_lowercase()
            || token1.trim() == token2.trim()
            || (is_identifier(token1)
            && is_identifier(token2)
            && token1.len() > 2
            && token2.len() > 2)
    }

    /// Integrate successful patterns into the permanent Patternizer collection
    pub fn integrate_successful_patterns(&mut self, context: &mut Context) -> Result<u32> {
        let mut integrated_count = 0;

        for (pattern_id, validation_result) in self.validation_results.iter() {
            if validation_result.success_rate > 0.9 {
                // High success threshold
                if let Some(pattern) = self.generated_patterns.get(pattern_id) {
                    self.integrate_pattern_into_patternizer(context, pattern)?;
                    integrated_count += 1;
                }
            }
        }

        self.integration_stats.patterns_integrated = integrated_count;
        self.integration_stats.success_rate =
            integrated_count as f64 / self.generated_patterns.len() as f64;

        report!(
            context,
            "samplizer",
            "integrate_successful_patterns",
            ReportLevel::Info,
            Phase::Process(Some("integrate_successful_patterns".to_string())),
            format!(
                "Integrated {} high-quality patterns into Patternizer",
                integrated_count
            ),
            true,
            0,
            0
        );

        Ok(integrated_count)
    }

    /// Integrate a single pattern into the Patternizer
    fn integrate_pattern_into_patternizer(
        &self,
        context: &mut Context,
        pattern: &SamplizerPattern,
    ) -> Result<()> {
        report!(
            context,
            "samplizer",
            "integrate_pattern_into_patternizer",
            ReportLevel::Info,
            Phase::Process(Some("integrate_pattern_into_patternizer".to_string())),
            format!(
                "Integrating pattern '{}' into Patternizer",
                pattern.pattern_name
            ),
            true,
            0,
            0
        );

        // Convert samplizer pattern to HandlerPattern
        if let Some(extraction_pattern) = &pattern.extraction_pattern {
            let handler_pattern =
                Self::convert_to_handler_pattern(pattern, extraction_pattern)?;

            // Register the pattern with appropriate handler type
            let handler_type = match pattern.pattern_type {
                SegmentType::Function => "function",
                SegmentType::Struct => "struct",
                SegmentType::Enum => "enum",
                SegmentType::Variable => "variable",
                SegmentType::TypeDef => "typedef",
                SegmentType::Macro => "macro",
                SegmentType::Array => "array",
                _ => "general",
            };

            context
                .patternizer
                .register_pattern(Id::get(&gen_name(handler_type)), handler_pattern);

            report!(
                context,
                "samplizer",
                "integrate_pattern_into_patternizer",
                ReportLevel::Info,
                Phase::Process(Some("integrate_pattern_into_patternizer".to_string())),
                format!(
                    "Successfully registered {} pattern with type '{}'",
                    pattern.pattern_name, handler_type
                ),
                true,
                0,
                0
            );
        }

        Ok(())
    }

    /// Convert samplizer extraction pattern to HandlerPattern
    fn convert_to_handler_pattern(
        pattern: &SamplizerPattern,
        extraction_pattern: &str,
    ) -> Result<HandlerPattern> {
        use crate::pattern::{HandlerPattern, PatternRule, TokenPattern};

        // Parse extraction pattern into TokenPattern rules
        let pattern_tokens: Vec<&str> = extraction_pattern.split_whitespace().collect();
        let mut rules = Vec::new();

        for token in pattern_tokens {
            let rule = match token {
                "IDENTIFIER" => PatternRule::new(TokenPattern::Identifier),
                "NUMBER" => PatternRule::new(TokenPattern::Number),
                "PREPROCESSOR" => PatternRule::new(TokenPattern::CKeyword),
                token_str if token_str.chars().all(|c| c.is_alphabetic()) => {
                    // Check if it's a type keyword
                    if matches!(
                        token_str,
                        "int"
                            | "char"
                            | "float"
                            | "double"
                            | "void"
                            | "struct"
                            | "enum"
                            | "typedef"
                            | "static"
                            | "inline"
                            | "extern"
                    ) {
                        PatternRule::new(TokenPattern::TypeKeyword)
                    } else {
                        PatternRule::new(TokenPattern::Exact(token_str.to_string()))
                    }
                }
                _ => PatternRule::new(TokenPattern::Exact(token.to_string())),
            };
            rules.push(rule);
        }

        // Create HandlerPattern with appropriate priority based on confidence
        let priority = (pattern.confidence_score * 300.0) as i32; // Scale to 0-300 range
        let min_tokens = rules.len().max(2);

        Ok(HandlerPattern::new(pattern.pattern_id.clone())
            .with_rules(rules)
            .min_tokens(min_tokens)
            .priority(priority))
    }

    /// Get statistics about the Samplizer's performance
    pub fn get_statistics(&self) -> &IntegrationStats {
        &self.integration_stats
    }

    /// Manually trigger learning from existing cache (useful for initialization)
    pub fn learn_from_existing_cache(&mut self, context: &mut Context) -> Result<()> {
        report!(
            context,
            "samplizer",
            "learn_from_existing_cache",
            ReportLevel::Info,
            Phase::Process(Some("learn_from_existing_cache".to_string())),
            "Starting to learn from existing cache data",
            true,
            0,
            0
        );

        self.analyze_existing_cache_results(context)?;
        self.save_learning_data(context)?;

        let positive_count: i32 = self.token_analysis_data.positive_patterns.values().sum();
        let negative_count: i32 = self.token_analysis_data.negative_patterns.values().sum();
        let total_tokens = self.token_analysis_data.token_success_rates.len();

        report!(
            context,
            "samplizer",
            "learn_from_existing_cache",
            ReportLevel::Info,
            Phase::Process(Some("learn_from_existing_cache".to_string())),
            format!(
                "Learning complete: {} positive patterns, {} negative patterns, {} unique tokens",
                positive_count, negative_count, total_tokens
            ),
            true,
            0,
            0
        );

        Ok(())
    }

    /// Enhanced token analysis with machine learning-based confidence calculation  
    pub fn analyze_tokens(&mut self, tokens: &[Token]) -> Result<ConfidenceResult> {
        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();
        let mut confidence_result = ConfidenceResult {
            overall_confidence: 0.5, // Base confidence
            token_contribution: 0.0,
            pattern_contribution: 0.0,
            historical_contribution: 0.0,
            reasons: Vec::new(),
        };

        // 1. Token-level analysis
        confidence_result.token_contribution = self.calculate_token_confidence(&token_strings);

        // 2. Pattern cache analysis
        confidence_result.pattern_contribution =
            self.calculate_pattern_cache_confidence(&token_strings)?;

        // 3. Historical learning analysis
        confidence_result.historical_contribution =
            self.calculate_historical_confidence(&token_strings);

        // 4. Combine all factors with weighted average
        confidence_result.overall_confidence = (0.3 * confidence_result.token_contribution)
            + (0.4 * confidence_result.pattern_contribution)
            + (0.3 * confidence_result.historical_contribution);

        // Ensure confidence stays in valid range
        confidence_result.overall_confidence =
            confidence_result.overall_confidence.max(0.0).min(1.0);

        Ok(confidence_result)
    }

    /// Calculate confidence based on individual tokens
    fn calculate_token_confidence(&self, tokens: &[String]) -> f64 {
        if tokens.is_empty() {
            return 0.0;
        }

        let mut total_confidence = 0.0;
        let mut evaluated_tokens = 0;

        for token in tokens {
            if let Some(&success_rate) = self.token_analysis_data.token_success_rates.get(token) {
                total_confidence += success_rate;
                evaluated_tokens += 1;
            } else {
                // New token - assign default confidence based on characteristics
                let default_confidence = self.get_default_token_confidence(token);
                total_confidence += default_confidence;
                evaluated_tokens += 1;
            }
        }

        if evaluated_tokens > 0 {
            total_confidence / evaluated_tokens as f64
        } else {
            0.5 // Default if no tokens evaluated
        }
    }
    /// Calculate confidence based on pattern cache matches
    fn calculate_pattern_cache_confidence(&self, tokens: &[String]) -> Result<f64> {
        let mut total_confidence = 0.0;
        let mut pattern_count = 0;

        // Check against common pattern types
        let pattern_names = [
            "function_declaration",
            "function_definition",
            "struct_declaration",
            "enum_declaration",
        ];

        for pattern_name in &pattern_names {
            pattern_count += 1;

            // Create pattern key for cache lookup
            let pattern_key = format!("{}:{}", pattern_name, tokens.join("|"));

            // Check if we have cached results for this pattern combination
            if let Some(&cached_score) = self
                .token_analysis_data
                .pattern_combinations
                .get(&pattern_key)
            {
                total_confidence += cached_score;
            } else {
                // Use token-based heuristic for unknown patterns
                let heuristic_confidence = self.calculate_pattern_heuristic(pattern_name, tokens);
                total_confidence += heuristic_confidence;
            }
        }

        let final_confidence = if pattern_count > 0 {
            total_confidence / pattern_count as f64
        } else {
            0.5
        };

        Ok(final_confidence.clamp(0.0, 1.0))
    }

    /// Calculate heuristic confidence for pattern matching
    fn calculate_pattern_heuristic(&self, pattern_name: &str, tokens: &[String]) -> f64 {
        match pattern_name {
            "function_declaration" => {
                if tokens
                    .iter()
                    .any(|t| matches!(t.as_str(), "int" | "void" | "char" | "float"))
                    && tokens.iter().any(|t| t == "(")
                    && tokens.iter().any(|t| t == ")")
                {
                    0.8
                } else {
                    0.3
                }
            }
            "function_definition" => {
                if tokens.iter().any(|t| t == "{") && tokens.iter().any(|t| t == "}") {
                    0.7
                } else {
                    0.2
                }
            }
            "struct_declaration" => {
                if tokens.iter().any(|t| t == "struct") {
                    0.9
                } else {
                    0.1
                }
            }
            "enum_declaration" => {
                if tokens.iter().any(|t| t == "enum") {
                    0.9
                } else {
                    0.1
                }
            }
            _ => 0.4,
        }
    }

    /// Calculate confidence based on historical positive/negative results
    fn calculate_historical_confidence(&self, tokens: &[String]) -> f64 {
        let token_sequence = tokens.join(" ");

        // Check sequence scores
        if let Some(&sequence_score) = self
            .token_analysis_data
            .sequence_scores
            .get(&token_sequence)
        {
            return sequence_score;
        }

        // Check individual token patterns
        let mut positive_score = 0.0;
        let mut negative_score = 0.0;

        for token in tokens {
            if let Some(&positive_count) = self.token_analysis_data.positive_patterns.get(token) {
                positive_score += positive_count as f64 * 0.1;
            }

            if let Some(&negative_count) = self.token_analysis_data.negative_patterns.get(token) {
                negative_score += negative_count as f64 * 0.1;
            }
        }

        // Convert to confidence (positive bias with negative penalty)
        let net_score = positive_score - (negative_score * 0.5);
        let confidence = 0.5 + (net_score / (tokens.len() as f64 + 1.0));

        confidence.max(0.0).min(1.0)
    }

    /// Get default confidence for unknown tokens based on characteristics
    fn get_default_token_confidence(&self, token: &str) -> f64 {
        match token {
            // High confidence for common C keywords
            "int" | "char" | "void" | "struct" | "enum" | "return" | "if" | "for" | "while" => 0.9,
            // Medium confidence for operators and punctuation
            "(" | ")" | "{" | "}" | ";" | "," | "=" => 0.7,
            // Lower confidence for unknown identifiers
            _ => {
                if token.chars().all(|c| c.is_alphanumeric() || c == '_') {
                    0.6 // Likely identifier
                } else {
                    0.4 // Unknown pattern
                }
            }
        }
    }

    /// Analyze pattern confidence for given tokens and pattern name
    pub fn analyze_pattern_confidence(&self, pattern_name: &str, tokens: &[Token]) -> Result<f64> {
        // Use existing token analysis with pattern-specific scoring
        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();

        // Base confidence from token success rates
        let mut confidence = 0.0;
        let mut token_count = 0;

        for token_str in &token_strings {
            if let Some(&success_rate) = self.token_analysis_data.token_success_rates.get(token_str)
            {
                confidence += success_rate;
            } else {
                // Default confidence for unknown tokens
                confidence += 0.4;
            }
            token_count += 1;
        }

        // Pattern-specific adjustments
        let pattern_key = format!("{}:{}", pattern_name, token_strings.join("|"));
        if let Some(&pattern_score) = self
            .token_analysis_data
            .pattern_combinations
            .get(&pattern_key)
        {
            confidence = (confidence + pattern_score) / 2.0; // Blend scores
        }

        // Check for negative patterns to avoid
        if self
            .token_analysis_data
            .negative_patterns
            .contains_key(&pattern_key)
        {
            confidence *= 0.6; // Reduce confidence for known bad patterns
        }

        // Check for positive reinforcement
        if let Some(&positive_count) = self.token_analysis_data.positive_patterns.get(&pattern_key)
        {
            confidence += (positive_count as f64 * 0.1).min(0.3); // Boost up to 0.3
        }

        let final_confidence = if token_count > 0 {
            (confidence / token_count as f64).clamp(0.0, 1.0)
        } else {
            0.1
        };

        Ok(final_confidence)
    }

    /// Record successful pattern matching for machine learning
    pub fn record_success_feedback(
        &mut self,
        pattern_name: &str,
        tokens: &[Token],
        confidence: f64,
    ) {
        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();

        // Update token success rates
        for token_str in &token_strings {
            let current_rate = self.token_analysis_data.token_success_rates
                .get(token_str)
                .copied()
                .unwrap_or(0.5);
            let updated_rate = (current_rate * 0.9 + confidence * 0.1).clamp(0.0, 1.0);
            self.token_analysis_data.token_success_rates
                .insert(token_str.clone(), updated_rate);
        }

        // Update pattern combinations
        let pattern_key = format!("{}:{}", pattern_name, token_strings.join("|"));
        let current_score = self.token_analysis_data.pattern_combinations
            .get(&pattern_key)
            .copied()
            .unwrap_or(0.5);
        let updated_score = (current_score * 0.8 + confidence * 0.2).clamp(0.0, 1.0);
        self.token_analysis_data.pattern_combinations
            .insert(pattern_key.clone(), updated_score);

        // Increment positive pattern counter
        let positive_count = self.token_analysis_data.positive_patterns
            .get(&pattern_key)
            .copied()
            .unwrap_or(0);
        self.token_analysis_data.positive_patterns
            .insert(pattern_key, positive_count + 1);
    }

    /// Record failed pattern matching to learn negative patterns
    pub fn record_failure_feedback(
        &mut self,
        pattern_name: &str,
        tokens: &[Token],
        _reason: &str,
    ) {
        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();
        let pattern_key = format!("{}:{}", pattern_name, token_strings.join("|"));

        // Increment negative pattern counter
        let negative_count = self.token_analysis_data.negative_patterns
            .get(&pattern_key)
            .copied()
            .unwrap_or(0);
        self.token_analysis_data.negative_patterns
            .insert(pattern_key.clone(), negative_count + 1);

        // Slightly reduce token success rates for failed patterns
        for token_str in &token_strings {
            let current_rate = self.token_analysis_data.token_success_rates
                .get(token_str)
                .copied()
                .unwrap_or(0.5);
            let updated_rate = (current_rate * 0.95).clamp(0.0, 1.0);
            self.token_analysis_data.token_success_rates
                .insert(token_str.clone(), updated_rate);
        }
    }

    /// Record conversion quality for continuous improvement
    pub fn record_conversion_quality(
        &mut self,
        pattern_name: &str,
        tokens: &[Token],
        quality_score: f64,
    ) {
        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();
        let pattern_key = format!("{}:{}", pattern_name, token_strings.join("|"));

        // Update sequence scores
        self.token_analysis_data.sequence_scores
            .insert(pattern_key, quality_score);
    }

    /// Get learned confidence based on historical data
    pub fn get_learned_confidence(&self, pattern_name: &str, tokens: &[Token]) -> f64 {
        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();
        let pattern_key = format!("{}:{}", pattern_name, token_strings.join("|"));

        // Check historical success
        let positive_count = self.token_analysis_data.positive_patterns
            .get(&pattern_key)
            .copied()
            .unwrap_or(0) as f64;
        let negative_count = self.token_analysis_data.negative_patterns
            .get(&pattern_key)
            .copied()
            .unwrap_or(0) as f64;

        if positive_count + negative_count > 0.0 {
            positive_count / (positive_count + negative_count)
        } else {
            0.5 // No historical data
        }
    }

    /// Enhanced pattern analysis with capture system
    pub fn analyze_with_captures(
        &self,
        tokens: &[Token],
        pattern_name: &str,
    ) -> Result<AnalysisResult> {
        let mut captures = CaptureSystem::new(pattern_name);

        // Use both Patternizer and Samplizer for comprehensive analysis
        let pattern_result = self.match_pattern(pattern_name, tokens);

        // Use the enhanced Samplizer token analysis with machine learning
        let samplizer_result = self.analyze_pattern_confidence(pattern_name, tokens);
        let samplizer_confidence = match samplizer_result {
            Ok(result) => result,
            Err(_) => self
                .analyze_pattern_confidence(pattern_name, tokens)
                .unwrap_or(0.5),
        };

        match pattern_result {
            PatternResult::Match { consumed_tokens } => {
                // Extract captures based on pattern type and token analysis
                self.extract_captures_from_tokens(
                    tokens,
                    pattern_name,
                    &mut captures,
                    consumed_tokens,
                )?;

                // Combine pattern match with samplizer confidence
                let combined_confidence = (0.7 * 1.0) + (0.3 * samplizer_confidence);
                captures.set_metadata(
                    combined_confidence,
                    consumed_tokens,
                    PatternMatchType::Exact,
                );

                Ok(AnalysisResult::success(captures, combined_confidence))
            }
            PatternResult::Fuzzy { offsets } => {
                // Handle fuzzy matches with reduced confidence
                let consumed = offsets.iter().map(|r| r.len()).sum();
                self.extract_captures_from_tokens(tokens, pattern_name, &mut captures, consumed)?;

                let fuzzy_confidence = samplizer_confidence * 0.8;
                captures.set_metadata(fuzzy_confidence, consumed, PatternMatchType::Fuzzy);

                Ok(AnalysisResult::success(captures, fuzzy_confidence))
            }
            PatternResult::NoMatch { reason } => Ok(AnalysisResult::failure(reason)),
            _ => Ok(AnalysisResult::failure(
                "Pattern matching failed".to_string(),
            )),
        }
    }

    /// Enhanced pattern matching implementation for token sequences with improved accuracy
    fn match_pattern(&self, pattern_name: &str, tokens: &[Token]) -> PatternResult {
        if tokens.is_empty() {
            return PatternResult::NoMatch {
                reason: "No tokens provided".to_string(),
            };
        }

        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();

        // Pre-analyze tokens for better pattern matching accuracy
        let confidence_score = self.analyze_pattern_confidence(pattern_name, tokens)
            .unwrap_or(0.5);

        // Apply confidence threshold for early filtering
        if confidence_score < 0.2 {
            return PatternResult::NoMatch {
                reason: format!("Low confidence score: {:.2}", confidence_score),
            };
        }

        let result = match pattern_name {
            "function_declaration" | "function_definition" => {
                self.match_function_pattern(&token_strings)
            }
            "struct_declaration" | "struct_definition" => {
                self.match_struct_pattern(&token_strings)
            }
            "enum_declaration" => self.match_enum_pattern(&token_strings),
            "variable_declaration" => self.match_variable_pattern(&token_strings),
            "union_declaration" => self.match_union_pattern(&token_strings),
            "typedef_declaration" => self.match_typedef_pattern(&token_strings),
            "macro_definition" => self.match_macro_pattern(&token_strings),
            _ => PatternResult::NoMatch {
                reason: format!("Unknown pattern: {}", pattern_name),
            },
        };

        // Apply confidence weighting to successful matches
        match result {
            PatternResult::Match { consumed_tokens } => {
                if confidence_score > 0.8 {
                    PatternResult::Match { consumed_tokens }
                } else if confidence_score > 0.5 {
                    PatternResult::Fuzzy {
                        offsets: vec![0..consumed_tokens]
                    }
                } else {
                    PatternResult::NoMatch {
                        reason: format!("Insufficient confidence: {:.2}", confidence_score),
                    }
                }
            }
            other => other,
        }
    }

    fn match_function_pattern(&self, tokens: &[String]) -> PatternResult {
        let mut consumed = 0;
        let mut i = 0;

        // Look for optional storage class specifiers
        while i < tokens.len()
            && matches!(tokens[i].as_str(), "static" | "extern" | "inline" | "const")
        {
            consumed += 1;
            i += 1;
        }

        // Look for return type (int, void, char, etc.)
        if i < tokens.len()
            && matches!(
                tokens[i].as_str(),
                "int"
                    | "void"
                    | "char"
                    | "float"
                    | "double"
                    | "long"
                    | "short"
                    | "unsigned"
                    | "signed"
            )
        {
            consumed += 1;
            i += 1;
        }

        // Look for function name (identifier)
        if i < tokens.len() && is_identifier(&tokens[i]) {
            consumed += 1;
            i += 1;
        } else {
            return PatternResult::NoMatch {
                reason: "No function name found".to_string(),
            };
        }

        // Look for opening parenthesis
        if i < tokens.len() && tokens[i] == "(" {
            consumed += 1;
            i += 1;

            // Skip parameters until closing parenthesis
            let mut paren_depth = 1;
            while i < tokens.len() && paren_depth > 0 {
                if tokens[i] == "(" {
                    paren_depth += 1;
                } else if tokens[i] == ")" {
                    paren_depth -= 1;
                }
                consumed += 1;
                i += 1;
            }

            if paren_depth == 0 {
                // Check for function body (definition) or semicolon (declaration)
                if i < tokens.len() && tokens[i] == "{" {
                    consumed += 1;
                    PatternResult::Match {
                        consumed_tokens: consumed,
                    }
                } else if i < tokens.len() && tokens[i] == ";" {
                    consumed += 1;
                    PatternResult::Match {
                        consumed_tokens: consumed,
                    }
                } else {
                    PatternResult::Match {
                        consumed_tokens: consumed,
                    }
                }
            } else {
                PatternResult::NoMatch {
                    reason: "Unmatched parentheses in function".to_string(),
                }
            }
        } else {
            PatternResult::NoMatch {
                reason: "No opening parenthesis found".to_string(),
            }
        }
    }

    fn match_struct_pattern(&self, tokens: &[String]) -> PatternResult {
        let mut consumed = 0;
        let mut i = 0;

        // Look for optional typedef
        if i < tokens.len() && tokens[i] == "typedef" {
            consumed += 1;
            i += 1;
        }

        // Look for struct keyword
        if i < tokens.len() && tokens[i] == "struct" {
            consumed += 1;
            i += 1;
        } else {
            return PatternResult::NoMatch {
                reason: "No struct keyword found".to_string(),
            };
        }

        // Look for optional struct name
        if i < tokens.len() && is_identifier(&tokens[i]) {
            consumed += 1;
            i += 1;
        }

        // Look for opening brace, semicolon, or typedef alias
        if i < tokens.len() && tokens[i] == "{" {
            consumed += 1;
            // Skip to closing brace for complete struct definition
            let mut brace_depth = 1;
            i += 1;
            while i < tokens.len() && brace_depth > 0 {
                if tokens[i] == "{" {
                    brace_depth += 1;
                } else if tokens[i] == "}" {
                    brace_depth -= 1;
                }
                consumed += 1;
                i += 1;
            }
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        } else if i < tokens.len() && tokens[i] == ";" {
            consumed += 1;
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        } else {
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        }
    }

    fn match_enum_pattern(&self, tokens: &[String]) -> PatternResult {
        let mut consumed = 0;
        let mut i = 0;

        // Look for optional typedef
        if i < tokens.len() && tokens[i] == "typedef" {
            consumed += 1;
            i += 1;
        }

        // Look for enum keyword
        if i < tokens.len() && tokens[i] == "enum" {
            consumed += 1;
            i += 1;
        } else {
            return PatternResult::NoMatch {
                reason: "No enum keyword found".to_string(),
            };
        }

        // Look for optional enum name
        if i < tokens.len() && is_identifier(&tokens[i]) {
            consumed += 1;
            i += 1;
        }

        // Look for opening brace, semicolon, or typedef alias
        if i < tokens.len() && tokens[i] == "{" {
            consumed += 1;
            // Skip to closing brace for complete enum definition
            let mut brace_depth = 1;
            i += 1;
            while i < tokens.len() && brace_depth > 0 {
                if tokens[i] == "{" {
                    brace_depth += 1;
                } else if tokens[i] == "}" {
                    brace_depth -= 1;
                }
                consumed += 1;
                i += 1;
            }
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        } else if i < tokens.len() && tokens[i] == ";" {
            consumed += 1;
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        } else {
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        }
    }

    fn match_variable_pattern(&self, tokens: &[String]) -> PatternResult {
        let mut consumed = 0;
        let mut i = 0;

        // Look for optional storage class specifiers
        while i < tokens.len()
            && matches!(
                tokens[i].as_str(),
                "static" | "extern" | "const" | "volatile"
            )
        {
            consumed += 1;
            i += 1;
        }

        // Look for type specifier
        if i < tokens.len()
            && matches!(
                tokens[i].as_str(),
                "int"
                    | "char"
                    | "float"
                    | "double"
                    | "void"
                    | "long"
                    | "short"
                    | "unsigned"
                    | "signed"
            )
        {
            consumed += 1;
            i += 1;
        } else if i < tokens.len() && is_identifier(&tokens[i]) {
            // Custom type
            consumed += 1;
            i += 1;
        } else {
            return PatternResult::NoMatch {
                reason: "No type specifier found".to_string(),
            };
        }

        // Look for variable name
        if i < tokens.len() && is_identifier(&tokens[i]) {
            consumed += 1;
            i += 1;

            // Look for array brackets or assignment
            if i < tokens.len() && tokens[i] == "[" {
                consumed += 1;
                i += 1;
                // Skip array size
                while i < tokens.len() && tokens[i] != "]" {
                    consumed += 1;
                    i += 1;
                }
                if i < tokens.len() && tokens[i] == "]" {
                    consumed += 1;
                    i += 1;
                }
            }

            // Look for assignment or semicolon
            if i < tokens.len() && tokens[i] == "=" {
                consumed += 1;
                i += 1;
                // Skip initialization value until semicolon
                while i < tokens.len() && tokens[i] != ";" {
                    consumed += 1;
                    i += 1;
                }
            }

            if i < tokens.len() && tokens[i] == ";" {
                consumed += 1;
                PatternResult::Match {
                    consumed_tokens: consumed,
                }
            } else {
                PatternResult::Match {
                    consumed_tokens: consumed,
                }
            }
        } else {
            PatternResult::NoMatch {
                reason: "No variable name found".to_string(),
            }
        }
    }

    fn match_union_pattern(&self, tokens: &[String]) -> PatternResult {
        let mut consumed = 0;
        let mut i = 0;

        // Look for optional typedef
        if i < tokens.len() && tokens[i] == "typedef" {
            consumed += 1;
            i += 1;
        }

        // Look for union keyword
        if i < tokens.len() && tokens[i] == "union" {
            consumed += 1;
            i += 1;
        } else {
            return PatternResult::NoMatch {
                reason: "No union keyword found".to_string(),
            };
        }

        // Look for optional union name
        if i < tokens.len() && is_identifier(&tokens[i]) {
            consumed += 1;
            i += 1;
        }

        // Look for opening brace or semicolon
        if i < tokens.len() && tokens[i] == "{" {
            consumed += 1;
            let mut brace_depth = 1;
            i += 1;
            while i < tokens.len() && brace_depth > 0 {
                if tokens[i] == "{" {
                    brace_depth += 1;
                } else if tokens[i] == "}" {
                    brace_depth -= 1;
                }
                consumed += 1;
                i += 1;
            }
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        } else if i < tokens.len() && tokens[i] == ";" {
            consumed += 1;
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        } else {
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        }
    }

    fn match_typedef_pattern(&self, tokens: &[String]) -> PatternResult {
        let mut consumed = 0;
        let mut i = 0;

        // Look for typedef keyword
        if i < tokens.len() && tokens[i] == "typedef" {
            consumed += 1;
            i += 1;
        } else {
            return PatternResult::NoMatch {
                reason: "No typedef keyword found".to_string(),
            };
        }

        // Look for existing type or struct/union/enum
        if i < tokens.len() && matches!(tokens[i].as_str(), "struct" | "union" | "enum") {
            consumed += 1;
            i += 1;

            // Optional tag name
            if i < tokens.len() && is_identifier(&tokens[i]) {
                consumed += 1;
                i += 1;
            }

            // Handle complete definition
            if i < tokens.len() && tokens[i] == "{" {
                consumed += 1;
                let mut brace_depth = 1;
                i += 1;
                while i < tokens.len() && brace_depth > 0 {
                    if tokens[i] == "{" {
                        brace_depth += 1;
                    } else if tokens[i] == "}" {
                        brace_depth -= 1;
                    }
                    consumed += 1;
                    i += 1;
                }
            }
        } else if i < tokens.len()
            && (is_identifier(&tokens[i])
            || matches!(
                    tokens[i].as_str(),
                    "int" | "char" | "float" | "double" | "void" | "long" | "short"
                ))
        {
            consumed += 1;
            i += 1;
        }

        // Look for new type name
        if i < tokens.len() && is_identifier(&tokens[i]) {
            consumed += 1;
            i += 1;
        }

        // Look for semicolon
        if i < tokens.len() && tokens[i] == ";" {
            consumed += 1;
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        } else {
            PatternResult::Match {
                consumed_tokens: consumed,
            }
        }
    }

    fn match_macro_pattern(&self, tokens: &[String]) -> PatternResult {
        let mut consumed = 0;
        let mut i = 0;

        // Look for #define
        if i < tokens.len() && tokens[i] == "#define" {
            consumed += 1;
            i += 1;
        } else {
            return PatternResult::NoMatch {
                reason: "No #define found".to_string(),
            };
        }

        // Look for macro name
        if i < tokens.len() && is_identifier(&tokens[i]) {
            consumed += 1;
            i += 1;

            // Handle function-like macros
            if i < tokens.len() && tokens[i] == "(" {
                consumed += 1;
                i += 1;
                let mut paren_depth = 1;
                while i < tokens.len() && paren_depth > 0 {
                    if tokens[i] == "(" {
                        paren_depth += 1;
                    } else if tokens[i] == ")" {
                        paren_depth -= 1;
                    }
                    consumed += 1;
                    i += 1;
                }
            }

            // Consume rest of macro definition until newline or end
            while i < tokens.len() && tokens[i] != "\n" {
                consumed += 1;
                i += 1;
            }

            PatternResult::Match {
                consumed_tokens: consumed,
            }
        } else {
            PatternResult::NoMatch {
                reason: "No macro name found".to_string(),
            }
        }
    }
    /// Helper function for samplizer-based analysis (moved from BaseHandler)
    pub fn analyze_with_range(
        &self,
        patterns: &Vec<String>,
        tokens: &[Token],
        token_range: Range<usize>,
    ) -> Result<f64> {
        // Early validation with more comprehensive checks
        if patterns.is_empty() || tokens.is_empty() {
            return Ok(0.0);
        }

        // Validate and safely extract tokens within range
        let range_tokens = match (token_range.start, token_range.end) {
            (start, end) if start >= tokens.len() => return Ok(0.0),
            (start, end) if end > tokens.len() => &tokens[start..],
            (start, end) if start < end => &tokens[token_range.clone()],
            _ => return Ok(0.0),
        };

        if range_tokens.is_empty() {
            return Ok(0.0);
        }

        let mut confidence_scores = Vec::with_capacity(patterns.len());

        for pattern_name in patterns {
            // Multi-source confidence calculation with error handling
            let pattern_confidence = match self.analyze_pattern_confidence(pattern_name, range_tokens) {
                Ok(conf) => conf,
                Err(_) => {
                    // Fallback to learned confidence
                    let learned = self.get_learned_confidence(pattern_name, range_tokens);
                    learned
                }
            };

            // Enhanced token-level analysis
            let token_strings: Vec<String> = range_tokens.iter().map(|t| t.to_string()).collect();
            let token_confidence = self.calculate_token_confidence(&token_strings);

            // Pattern cache analysis for additional context
            let pattern_cache_confidence = self.calculate_pattern_cache_confidence(&token_strings).unwrap_or_else(|_| 0.5);

            // Context-aware confidence blending
            let blended_confidence = (0.5 * pattern_confidence)
                + (0.3 * token_confidence)
                + (0.2 * pattern_cache_confidence);

            confidence_scores.push(blended_confidence);
        }

        // Calculate average confidence with range quality adjustment
        let average_confidence = if !confidence_scores.is_empty() {
            confidence_scores.iter().sum::<f64>() / confidence_scores.len() as f64
        } else {
            0.1
        };

        // Apply range-specific adjustments based on token coverage
        let range_factor = if tokens.len() > 0 {
            1.0 - (token_range.len() as f64 / tokens.len() as f64 * 0.1)
        } else {
            1.0
        };

        let final_confidence = (average_confidence * range_factor).clamp(0.0, 1.0);

        Ok(final_confidence)
    }
    fn extract_captures_from_tokens(
        &self,
        tokens: &[Token],
        pattern_name: &str,
        captures: &mut CaptureSystem,
        consumed_tokens: usize,
    ) -> Result<()> {
        // Enhanced input validation with detailed error handling
        if consumed_tokens == 0 {
            captures.capture_text("empty_pattern", "true".to_string(), 1.0);
            return Ok(());
        }

        if tokens.is_empty() {
            captures.capture_text("no_tokens", "true".to_string(), 1.0);
            return Ok(());
        }

        let safe_consumed = consumed_tokens.min(tokens.len());
        let token_strings: Vec<String> = tokens
            .iter()
            .take(safe_consumed)
            .map(|t| t.to_string())
            .collect();

        // Enhanced metadata capture with additional context
        captures.capture_text("pattern_type", pattern_name.to_string(), 1.0);
        captures.capture_text("token_count", safe_consumed.to_string(), 1.0);
        captures.capture_text(
            "pattern_complexity",
            self.calculate_pattern_complexity(&token_strings).to_string(),
            1.0,
        );

        // Add token position and context analysis
        captures.capture_range("consumed_range", 0..safe_consumed, 0.95);
        if let Some(first_token) = token_strings.first() {
            captures.capture_text("leading_token", first_token.clone(), 0.9);
        }
        if let Some(last_token) = token_strings.last() {
            captures.capture_text("trailing_token", last_token.clone(), 0.9);
        }

        // Pattern-specific extraction with improved error handling and fallback strategy
        let extraction_result = match pattern_name {
            "function_declaration" | "function_definition" => {
                self.extract_function_captures(tokens, captures, safe_consumed)
            }
            "struct_declaration" | "struct_definition" => {
                self.extract_struct_captures(tokens, captures, safe_consumed)
            }
            "enum_declaration" | "enum_definition" => {
                self.extract_enum_captures(tokens, captures, safe_consumed)
            }
            "union_declaration" | "union_definition" => {
                self.extract_union_captures(tokens, captures, safe_consumed)
            }
            "variable_declaration" | "variable_definition" => {
                self.extract_variable_captures(tokens, captures, safe_consumed)
            }
            "typedef_declaration" => {
                self.extract_typedef_captures(tokens, captures, safe_consumed)
            }
            "macro_definition" | "macro_declaration" => {
                self.extract_macro_captures(tokens, captures, safe_consumed)
            }
            _ => {
                // Mark as generic pattern for better tracking
                captures.capture_text("generic_pattern", "true".to_string(), 0.8);
                self.extract_fallback_captures(&token_strings, captures, safe_consumed)
            }
        };

        // Handle extraction errors gracefully with enhanced error context
        if let Err(e) = extraction_result {
            captures.capture_text("extraction_error", e.to_string(), 0.5);
            captures.capture_text("error_recovery", "fallback_extraction".to_string(), 0.7);

            // Attempt fallback extraction with error context
            if let Err(fallback_error) = self.extract_fallback_captures(&token_strings, captures, safe_consumed) {
                captures.capture_text("fallback_error", fallback_error.to_string(), 0.3);
            }
        }

        // Final validation and confidence adjustment
        let extraction_confidence = if captures.is_empty() { 0.3 } else { 0.9 };
        captures.capture_text("extraction_confidence", extraction_confidence.to_string(), 1.0);

        Ok(())
    }

    /// Enhanced fallback extraction with comprehensive pattern analysis
    fn extract_fallback_captures(
        &self,
        token_strings: &[String],
        captures: &mut CaptureSystem,
        consumed_tokens: usize,
    ) -> Result<()> {
        // Basic token information with enhanced metadata
        captures.capture_text("raw_tokens", token_strings.join(" "), 0.7);
        captures.capture_range("token_range", 0..consumed_tokens, 0.9);
        captures.capture_text("token_count", consumed_tokens.to_string(), 0.85);

        // Enhanced boundary token analysis with context
        if let Some(first_token) = token_strings.first() {
            captures.capture_text("first_token", first_token.clone(), 0.9);

            // Classify first token type with expanded categories
            if is_identifier(first_token) {
                captures.capture_text("primary_identifier", first_token.clone(), 0.85);
                captures.capture_text("starts_with_identifier", "true".to_string(), 0.9);
            } else if matches!(first_token.as_str(), "if" | "while" | "for" | "switch" | "do") {
                captures.capture_text("control_flow_start", first_token.clone(), 0.9);
            } else if matches!(first_token.as_str(), "int" | "char" | "float" | "double" | "void" | "struct" | "enum" | "union") {
                captures.capture_text("type_declaration_start", first_token.clone(), 0.9);
            } else if first_token == "#" || first_token.starts_with("#") {
                captures.capture_text("preprocessor_directive", "true".to_string(), 0.95);
            }
        }

        if let Some(last_token) = token_strings.last() {
            captures.capture_text("last_token", last_token.clone(), 0.8);
            match last_token.as_str() {
                ";" => {
                    captures.capture_text("statement_complete", "true".to_string(), 0.95);
                    captures.capture_text("ends_with_semicolon", "true".to_string(), 0.9);
                }
                "{" => {
                    captures.capture_text("block_start", "true".to_string(), 0.9);
                    captures.capture_text("compound_statement_start", "true".to_string(), 0.85);
                }
                "}" => {
                    captures.capture_text("block_end", "true".to_string(), 0.9);
                    captures.capture_text("compound_statement_end", "true".to_string(), 0.85);
                }
                ")" => captures.capture_text("expression_end", "true".to_string(), 0.8),
                _ => {}
            }
        }

        // Structural analysis
        let brace_count = token_strings.iter().filter(|&t| t == "{" || t == "}").count();
        let paren_count = token_strings.iter().filter(|&t| t == "(" || t == ")").count();
        let bracket_count = token_strings.iter().filter(|&t| t == "[" || t == "]").count();

        if brace_count > 0 {
            captures.capture_text("has_braces", "true".to_string(), 0.9);
            captures.capture_text("brace_count", brace_count.to_string(), 0.8);
        }
        if paren_count > 0 {
            captures.capture_text("has_parentheses", "true".to_string(), 0.9);
            captures.capture_text("paren_count", paren_count.to_string(), 0.8);
        }
        if bracket_count > 0 {
            captures.capture_text("has_brackets", "true".to_string(), 0.9);
            captures.capture_text("bracket_count", bracket_count.to_string(), 0.8);
        }

        // Identifier analysis
        let identifiers: Vec<String> = token_strings.iter()
            .filter(|&t| is_identifier(t))
            .cloned()
            .collect();

        if !identifiers.is_empty() {
            captures.capture_text("identifier_count", identifiers.len().to_string(), 0.85);
            captures.capture_text("identifiers", identifiers.join(", "), 0.75);
        }

        // Advanced pattern detection and generic extraction
        self.extract_generic_patterns(token_strings, captures)?;

        Ok(())
    }

    /// Calculate pattern complexity based on token analysis
    fn calculate_pattern_complexity(&self, tokens: &[String]) -> usize {
        let mut complexity = 0;

        for token in tokens {
            match token.as_str() {
                "{" | "}" | "(" | ")" | "[" | "]" => complexity += 1,
                "if" | "while" | "for" | "switch" => complexity += 2,
                _ if is_identifier(token) => complexity += 1,
                _ => {}
            }
        }

        complexity
    }

    /// Extract advanced patterns from token sequences
    fn extract_advanced_patterns(&self, tokens: &[String], captures: &mut CaptureSystem) -> Result<()> {
        // Detect nested structures
        let brace_count = tokens.iter().filter(|&t| t == "{" || t == "}").count();
        if brace_count > 0 {
            captures.capture_text("has_nested_structure", "true".to_string(), 0.8);
            captures.capture_text("brace_count", brace_count.to_string(), 0.85);
        }

        // Detect function calls
        for (i, token) in tokens.iter().enumerate() {
            if i + 1 < tokens.len() && tokens[i + 1] == "(" {
                if is_identifier(token) {
                    captures.capture_text("contains_function_call", token.clone(), 0.8);
                }
            }
        }

        // Detect pointer usage
        if tokens.contains(&"*".to_string()) {
            captures.capture_text("contains_pointer", "true".to_string(), 0.85);
        }

        // Detect array usage
        if tokens.contains(&"[".to_string()) && tokens.contains(&"]".to_string()) {
            captures.capture_text("contains_array", "true".to_string(), 0.85);
        }

        Ok(())
    }

    /// Extract union-specific captures with enhanced member analysis
    fn extract_union_captures(
        &self,
        tokens: &[Token],
        captures: &mut CaptureSystem,
        consumed_tokens: usize,
    ) -> Result<()> {
        let token_strings: Vec<String> = tokens
            .iter()
            .take(consumed_tokens)
            .map(|t| t.to_string())
            .collect();

        // Find union name with better positioning
        if let Some(union_pos) = token_strings.iter().position(|t| t == "union") {
            if union_pos + 1 < token_strings.len() {
                let union_name = &token_strings[union_pos + 1];
                if is_identifier(union_name) {
                    captures.capture_text("union_name", union_name.clone(), 0.95);
                    captures.capture_range("union_name_pos", union_pos + 1..union_pos + 2, 0.95);
                }
            }
        }

        // Enhanced union member extraction
        if let Some(open_brace) = token_strings.iter().position(|t| t == "{") {
            if let Some(close_brace) = token_strings.iter().rposition(|t| t == "}") {
                if open_brace < close_brace {
                    let member_tokens = &token_strings[open_brace + 1..close_brace];
                    let members = self.extract_struct_fields(member_tokens);

                    if !members.is_empty() {
                        captures.capture_text("union_members", members.join("; "), 0.85);
                        captures.capture_text("member_count", members.len().to_string(), 0.9);

                        // Extract individual member details
                        for (i, member) in members.iter().enumerate() {
                            captures.capture_text(&format!("member_{}", i), member.clone(), 0.8);
                        }
                    }

                    captures.capture_text("union_body", "present".to_string(), 0.95);
                }
            }
        }

        // Check for typedef union pattern
        if let Some(typedef_pos) = token_strings.iter().position(|t| t == "typedef") {
            if typedef_pos + 1 < token_strings.len() && token_strings[typedef_pos + 1] == "union" {
                captures.capture_text("typedef_union", "true".to_string(), 0.9);

                // Look for typedef alias at the end
                if let Some(semicolon_pos) = token_strings.iter().rposition(|t| t == ";") {
                    if semicolon_pos > 0 {
                        let alias_name = &token_strings[semicolon_pos - 1];
                        if is_identifier(alias_name) {
                            captures.capture_text("typedef_alias", alias_name.clone(), 0.9);
                        }
                    }
                }
            }
        }

        Ok(())
    }

    /// Extract struct/union fields from field tokens
    fn extract_struct_fields(&self, field_tokens: &[String]) -> Vec<String> {
        let mut fields = Vec::new();
        let mut current_field = Vec::new();

        for token in field_tokens {
            match token.as_str() {
                ";" => {
                    if !current_field.is_empty() {
                        fields.push(current_field.join(" ").trim().to_string());
                        current_field.clear();
                    }
                }
                "," => {
                    if !current_field.is_empty() {
                        fields.push(current_field.join(" ").trim().to_string());
                        current_field.clear();
                    }
                }
                _ => {
                    current_field.push(token.clone());
                }
            }
        }

        // Handle last field if no trailing semicolon
        if !current_field.is_empty() {
            fields.push(current_field.join(" ").trim().to_string());
        }

        fields.into_iter().filter(|f| !f.is_empty()).collect()
    }

    /// Extract variable declaration captures
    fn extract_variable_captures(
        &self,
        tokens: &[Token],
        captures: &mut CaptureSystem,
        consumed_tokens: usize,
    ) -> Result<()> {
        let token_strings: Vec<String> = tokens
            .iter()
            .take(consumed_tokens)
            .map(|t| t.to_string())
            .collect();

        // Simple variable declaration: type name;
        let mut type_tokens = Vec::new();
        let mut var_name = String::new();
        let mut found_type = false;

        for (i, token) in token_strings.iter().enumerate() {
            if matches!(
                token.as_str(),
                "int"
                    | "char"
                    | "float"
                    | "double"
                    | "void"
                    | "long"
                    | "short"
                    | "unsigned"
                    | "signed"
            ) {
                type_tokens.push(token.clone());
                found_type = true;
            } else if found_type && is_identifier(token) && var_name.is_empty() {
                var_name = token.clone();
                break;
            } else if found_type {
                type_tokens.push(token.clone());
            }
        }

        if !type_tokens.is_empty() {
            captures.capture_text("variable_type", type_tokens.join(" "), 0.9);
        }

        if !var_name.is_empty() {
            captures.capture_text("variable_name", var_name, 0.95);
        }

        // Check for initialization
        if token_strings.contains(&"=".to_string()) {
            captures.capture_text("has_initializer", "true".to_string(), 0.9);
        }

        Ok(())
    }

    /// Extract typedef declaration captures
    fn extract_typedef_captures(
        &self,
        tokens: &[Token],
        captures: &mut CaptureSystem,
        consumed_tokens: usize,
    ) -> Result<()> {
        let token_strings: Vec<String> = tokens
            .iter()
            .take(consumed_tokens)
            .map(|t| t.to_string())
            .collect();

        if let Some(typedef_pos) = token_strings.iter().position(|t| t == "typedef") {
            captures.capture_text("is_typedef", "true".to_string(), 1.0);

            // Extract alias name (usually the last identifier before semicolon)
            if let Some(semicolon_pos) = token_strings.iter().rposition(|t| t == ";") {
                if semicolon_pos > typedef_pos + 1 {
                    let alias_name = &token_strings[semicolon_pos - 1];
                    if is_identifier(alias_name) {
                        captures.capture_text("typedef_alias", alias_name.clone(), 0.95);
                    }
                }
            }

            // Extract base type
            let type_tokens: Vec<String> = token_strings
                [typedef_pos + 1..token_strings.len().saturating_sub(2)]
                .iter()
                .cloned()
                .collect();

            if !type_tokens.is_empty() {
                captures.capture_text("typedef_base_type", type_tokens.join(" "), 0.85);
            }
        }

        Ok(())
    }

    /// Extract macro definition captures
    fn extract_macro_captures(
        &self,
        tokens: &[Token],
        captures: &mut CaptureSystem,
        consumed_tokens: usize,
    ) -> Result<()> {
        let token_strings: Vec<String> = tokens
            .iter()
            .take(consumed_tokens)
            .map(|t| t.to_string())
            .collect();

        if let Some(define_pos) = token_strings.iter().position(|t| t == "#define") {
            if define_pos + 1 < token_strings.len() {
                let macro_name = &token_strings[define_pos + 1];
                captures.capture_text("macro_name", macro_name.clone(), 0.95);

                // Check for function-like macro
                if define_pos + 2 < token_strings.len() && token_strings[define_pos + 2] == "(" {
                    captures.capture_text("is_function_macro", "true".to_string(), 0.9);

                    // Extract parameters
                    if let Some(close_paren) = token_strings.iter().position(|t| t == ")") {
                        let param_tokens = &token_strings[define_pos + 3..close_paren];
                        if !param_tokens.is_empty() {
                            captures.capture_text(
                                "macro_parameters",
                                param_tokens.join(", "),
                                0.85,
                            );
                        }
                    }
                } else {
                    captures.capture_text("is_object_macro", "true".to_string(), 0.9);
                }

                // Extract macro body
                let body_start = if token_strings.contains(&"(".to_string()) {
                    token_strings
                        .iter()
                        .position(|t| t == ")")
                        .map(|p| p + 1)
                        .unwrap_or(define_pos + 2)
                } else {
                    define_pos + 2
                };

                if body_start < token_strings.len() {
                    let body_tokens = &token_strings[body_start..];
                    if !body_tokens.is_empty() {
                        captures.capture_text("macro_body", body_tokens.join(" "), 0.8);
                    }
                }
            }
        }

        Ok(())
    }

    /// Extract generic patterns from unknown token sequences
    fn extract_generic_patterns(
        &self,
        token_strings: &[String],
        captures: &mut CaptureSystem,
    ) -> Result<()> {
        // Look for assignment patterns
        if let Some(equals_pos) = token_strings.iter().position(|t| t == "=") {
            if equals_pos > 0 && equals_pos < token_strings.len() - 1 {
                let lhs = &token_strings[0..equals_pos];
                let rhs = &token_strings[equals_pos + 1..];
                captures.capture_text("assignment_lhs", lhs.join(" "), 0.7);
                captures.capture_text("assignment_rhs", rhs.join(" "), 0.7);
            }
        }

        // Look for function call patterns
        if let Some(paren_pos) = token_strings.iter().position(|t| t == "(") {
            if paren_pos > 0 {
                let func_name = &token_strings[paren_pos - 1];
                if is_identifier(func_name) {
                    captures.capture_text("function_call", func_name.clone(), 0.75);
                }
            }
        }

        // Look for type patterns
        for (i, token) in token_strings.iter().enumerate() {
            if matches!(
                token.as_str(),
                "int"
                    | "char"
                    | "float"
                    | "double"
                    | "void"
                    | "long"
                    | "short"
                    | "unsigned"
                    | "signed"
            ) {
                captures.capture_text("type_specifier", token.clone(), 0.9);
                // Capture following identifier as variable name
                if i + 1 < token_strings.len() && is_identifier(&token_strings[i + 1]) {
                    captures.capture_text("variable_name", token_strings[i + 1].clone(), 0.85);
                }
                break;
            }
        }

        Ok(())
    }

    /// Extract function-specific captures with enhanced analysis
    fn extract_function_captures(
        &self,
        tokens: &[Token],
        captures: &mut CaptureSystem,
        consumed_tokens: usize,
    ) -> Result<()> {
        let token_strings: Vec<String> = tokens
            .iter()
            .take(consumed_tokens)
            .map(|t| t.to_string())
            .collect();

        // Enhanced function name detection
        for i in 0..token_strings.len().saturating_sub(1) {
            if token_strings[i + 1] == "(" {
                let potential_name = &token_strings[i];
                if is_identifier(potential_name) {
                    captures.capture_text("function_name", potential_name.clone(), 0.95);
                    captures.capture_range("function_name_pos", i..i + 1, 0.95);

                    // Enhanced return type extraction with better filtering
                    if i > 0 {
                        let return_type_tokens: Vec<String> = token_strings[0..i]
                            .iter()
                            .filter(|&t| {
                                !matches!(
                                    t.as_str(),
                                    "static" | "extern" | "inline" | "const" | "volatile"
                                )
                            })
                            .cloned()
                            .collect();

                        if !return_type_tokens.is_empty() {
                            captures.capture_text("return_type", return_type_tokens.join(" "), 0.9);
                        } else {
                            // Default to void if no return type found
                            captures.capture_text("return_type", "void".to_string(), 0.7);
                        }
                    }

                    // Enhanced parameter extraction with better parsing
                    if let Some(open_paren) = token_strings.iter().position(|t| t == "(") {
                        if let Some(close_paren) = token_strings.iter().position(|t| t == ")") {
                            if open_paren < close_paren {
                                let param_tokens = &token_strings[open_paren + 1..close_paren];
                                self.extract_function_parameters(param_tokens, captures)?;
                            }
                        }
                    }

                    break;
                }
            }
        }

        // Enhanced function attributes detection
        self.extract_function_attributes(&token_strings, captures)?;

        // Function complexity estimation
        let complexity = self.estimate_function_complexity(&token_strings);
        captures.capture_text("complexity", complexity, 0.8);

        Ok(())
    }

    /// Extract function parameters with detailed analysis
    fn extract_function_parameters(
        &self,
        param_tokens: &[String],
        captures: &mut CaptureSystem,
    ) -> Result<()> {
        if param_tokens.is_empty() || (param_tokens.len() == 1 && param_tokens[0] == "void") {
            captures.capture_text("parameter_count", "0".to_string(), 1.0);
            return Ok(());
        }

        // Split parameters by comma
        let mut parameters = Vec::new();
        let mut current_param = Vec::new();
        let mut paren_depth = 0;

        for token in param_tokens {
            match token.as_str() {
                "(" => {
                    paren_depth += 1;
                    current_param.push(token.clone());
                }
                ")" => {
                    paren_depth -= 1;
                    current_param.push(token.clone());
                }
                "," if paren_depth == 0 => {
                    if !current_param.is_empty() {
                        parameters.push(current_param.join(" "));
                        current_param.clear();
                    }
                }
                _ => current_param.push(token.clone()),
            }
        }

        // Add last parameter
        if !current_param.is_empty() {
            parameters.push(current_param.join(" "));
        }

        // Check for variadic function
        let is_variadic = parameters.iter().any(|p| p.contains("..."));
        if is_variadic {
            captures.capture_text("variadic", "true".to_string(), 1.0);
        }

        captures.capture_text("parameter_count", parameters.len().to_string(), 1.0);
        captures.capture_text("parameters", parameters.join("; "), 0.85);

        // Extract individual parameter details
        for (i, param) in parameters.iter().enumerate() {
            captures.capture_text(&format!("param_{}", i), param.clone(), 0.8);
        }

        Ok(())
    }

    /// Extract function attributes and modifiers
    fn extract_function_attributes(
        &self,
        token_strings: &[String],
        captures: &mut CaptureSystem,
    ) -> Result<()> {
        // Check for function body
        if token_strings.contains(&"{".to_string()) {
            captures.capture_text("function_body", "present".to_string(), 0.95);
            captures.capture_text("is_definition", "true".to_string(), 0.95);
        } else {
            captures.capture_text("is_declaration", "true".to_string(), 0.95);
        }

        // Storage class specifiers
        if token_strings.contains(&"static".to_string()) {
            captures.capture_text("static", "true".to_string(), 1.0);
        }
        if token_strings.contains(&"extern".to_string()) {
            captures.capture_text("extern", "true".to_string(), 1.0);
        }
        if token_strings.contains(&"inline".to_string()) {
            captures.capture_text("inline", "true".to_string(), 1.0);
        }

        // Type qualifiers
        if token_strings.contains(&"const".to_string()) {
            captures.capture_text("const", "true".to_string(), 1.0);
        }
        if token_strings.contains(&"volatile".to_string()) {
            captures.capture_text("volatile", "true".to_string(), 1.0);
        }

        Ok(())
    }

    /// Estimate function complexity based on token patterns
    fn estimate_function_complexity(&self, token_strings: &[String]) -> String {
        let control_flow_keywords = ["if", "else", "while", "for", "switch", "case", "do"];
        let control_flow_count = token_strings
            .iter()
            .filter(|t| control_flow_keywords.contains(&t.as_str()))
            .count();

        match control_flow_count {
            0..=2 => "low".to_string(),
            3..=7 => "medium".to_string(),
            _ => "high".to_string(),
        }
    }

    /// Extract struct-specific captures with enhanced field analysis
    fn extract_struct_captures(
        &self,
        tokens: &[Token],
        captures: &mut CaptureSystem,
        consumed_tokens: usize,
    ) -> Result<()> {
        let token_strings: Vec<String> = tokens
            .iter()
            .take(consumed_tokens)
            .map(|t| t.to_string())
            .collect();

        // Find struct name with better positioning
        if let Some(struct_pos) = token_strings.iter().position(|t| t == "struct") {
            if struct_pos + 1 < token_strings.len() {
                let struct_name = &token_strings[struct_pos + 1];
                if is_identifier(struct_name) {
                    captures.capture_text("struct_name", struct_name.clone(), 0.95);
                    captures.capture_range("struct_name_pos", struct_pos + 1..struct_pos + 2, 0.95);
                }
            }
        }

        // Enhanced struct field extraction
        if let Some(open_brace) = token_strings.iter().position(|t| t == "{") {
            if let Some(close_brace) = token_strings.iter().rposition(|t| t == "}") {
                if open_brace < close_brace {
                    let field_tokens = &token_strings[open_brace + 1..close_brace];
                    let fields = self.extract_struct_fields(field_tokens);

                    if !fields.is_empty() {
                        captures.capture_text("struct_fields", fields.join("; "), 0.85);
                        captures.capture_text("field_count", fields.len().to_string(), 0.9);

                        // Extract individual field details
                        for (i, field) in fields.iter().enumerate() {
                            captures.capture_text(&format!("field_{}", i), field.clone(), 0.8);
                        }
                    }

                    captures.capture_text("struct_body", "present".to_string(), 0.95);
                }
            }
        }

        // Check for typedef struct pattern
        if let Some(typedef_pos) = token_strings.iter().position(|t| t == "typedef") {
            if typedef_pos + 1 < token_strings.len() && token_strings[typedef_pos + 1] == "struct" {
                captures.capture_text("typedef_struct", "true".to_string(), 0.9);

                // Look for typedef alias at the end
                if let Some(semicolon_pos) = token_strings.iter().rposition(|t| t == ";") {
                    if semicolon_pos > 0 {
                        let alias = &token_strings[semicolon_pos - 1];
                        if is_identifier(alias) {
                            captures.capture_text("typedef_alias", alias.clone(), 0.9);
                        }
                    }
                }
            }
        }

        Ok(())
    }

    /// Extract enum-specific captures with enhanced value analysis
    fn extract_enum_captures(
        &self,
        tokens: &[Token],
        captures: &mut CaptureSystem,
        consumed_tokens: usize,
    ) -> Result<()> {
        let token_strings: Vec<String> = tokens
            .iter()
            .take(consumed_tokens)
            .map(|t| t.to_string())
            .collect();

        // Find enum name with better positioning
        if let Some(enum_pos) = token_strings.iter().position(|t| t == "enum") {
            if enum_pos + 1 < token_strings.len() {
                let enum_name = &token_strings[enum_pos + 1];
                if is_identifier(enum_name) {
                    captures.capture_text("enum_name", enum_name.clone(), 0.95);
                    captures.capture_range("enum_name_pos", enum_pos + 1..enum_pos + 2, 0.95);
                }
            }
        }

        // Enhanced enum field extraction with value analysis
        if let Some(open_brace) = token_strings.iter().position(|t| t == "{") {
            if let Some(close_brace) = token_strings.iter().rposition(|t| t == "}") {
                if open_brace < close_brace {
                    let field_tokens = &token_strings[open_brace + 1..close_brace];
                    let fields = self.extract_enum_fields(field_tokens);

                    if !fields.is_empty() {
                        captures.capture_text("enum_fields", fields.join("; "), 0.85);
                        captures.capture_text("enum_value_count", fields.len().to_string(), 0.9);

                        // Extract individual enum values with detailed analysis
                        for (i, field) in fields.iter().enumerate() {
                            captures.capture_text(&format!("enum_value_{}", i), field.clone(), 0.8);

                            // Check for explicit value assignments
                            if field.contains("=") {
                                let parts: Vec<&str> = field.split('=').collect();
                                if parts.len() == 2 {
                                    captures.capture_text(
                                        &format!("enum_value_{}_name", i),
                                        parts[0].trim().to_string(),
                                        0.85,
                                    );
                                    captures.capture_text(
                                        &format!("enum_value_{}_assignment", i),
                                        parts[1].trim().to_string(),
                                        0.85,
                                    );
                                }
                            }
                        }

                        // Analyze enum type characteristics
                        let has_explicit_values = fields.iter().any(|f| f.contains("="));
                        captures.capture_text(
                            "has_explicit_values",
                            has_explicit_values.to_string(),
                            0.9,
                        );
                    }

                    captures.capture_text("enum_body", "present".to_string(), 0.95);
                }
            }
        }

        // Check for typedef enum pattern
        if let Some(typedef_pos) = token_strings.iter().position(|t| t == "typedef") {
            if typedef_pos + 1 < token_strings.len() && token_strings[typedef_pos + 1] == "enum" {
                captures.capture_text("typedef_enum", "true".to_string(), 0.9);

                // Look for typedef alias at the end
                if let Some(semicolon_pos) = token_strings.iter().rposition(|t| t == ";") {
                    if semicolon_pos > 0 {
                        let alias = &token_strings[semicolon_pos - 1];
                        if is_identifier(alias) {
                            captures.capture_text("typedef_alias", alias.clone(), 0.9);
                        }
                    }
                }
            }
        }

        // Detect enum forward declaration
        if !token_strings.contains(&"{".to_string()) {
            captures.capture_text("is_forward_declaration", "true".to_string(), 0.9);
        } else {
            captures.capture_text("is_definition", "true".to_string(), 0.9);
        }

        Ok(())
    }

    /// Extract enum fields from tokens
    fn extract_enum_fields(&self, tokens: &[String]) -> Vec<String> {
        let mut fields = Vec::new();
        let mut current_field = Vec::new();
        let mut depth = 0;

        for token in tokens {
            match token.as_str() {
                "," if depth == 0 => {
                    if !current_field.is_empty() {
                        fields.push(current_field.join(" ").trim().to_string());
                        current_field.clear();
                    }
                }
                "(" | "{" | "[" => {
                    current_field.push(token.clone());
                    depth += 1;
                }
                ")" | "}" | "]" => {
                    current_field.push(token.clone());
                    depth -= 1;
                }
                _ => {
                    current_field.push(token.clone());
                }
            }
        }

        // Add the last field if it exists
        if !current_field.is_empty() {
            fields.push(current_field.join(" ").trim().to_string());
        }

        fields
    }
    /// Record failed pattern match for learning
    pub fn record_failure(
        &mut self,
        context: &mut Context,
        tokens: &[Token],
        pattern_name: &str,
    ) -> Result<()> {
        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();

        // Update individual token success rates (decrease)
        for token_str in &token_strings {
            let current_rate = self
                .token_analysis_data
                .token_success_rates
                .get(token_str)
                .unwrap_or(&0.5);
            let new_rate = (current_rate * 0.9) + (0.0 * 0.1); // Moving average with failure
            self.token_analysis_data
                .token_success_rates
                .insert(token_str.clone(), new_rate);
        }

        // Update negative patterns
        for token_str in &token_strings {
            *self
                .token_analysis_data
                .negative_patterns
                .entry(token_str.clone())
                .or_insert(0) += 1;
        }

        // Update sequence scores (decrease)
        let sequence = token_strings.join(" ");
        let current_score = self
            .token_analysis_data
            .sequence_scores
            .get(&sequence)
            .unwrap_or(&0.5);
        let new_score = (current_score * 0.8) + (0.0 * 0.2);
        self.token_analysis_data
            .sequence_scores
            .insert(sequence, new_score);

        self.save_learning_data(context)?;
        Ok(())
    }
    /// Record successful pattern match for learning
    pub fn record_success(
        &mut self,
        context: &mut Context,
        tokens: &[Token],
        pattern_name: &str,
    ) -> Result<()> {
        let token_strings: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();

        // Update individual token success rates
        for token_str in &token_strings {
            let current_rate = self
                .token_analysis_data
                .token_success_rates
                .get(token_str)
                .unwrap_or(&0.5);
            let new_rate = (current_rate * 0.9) + (1.0 * 0.1); // Moving average with success
            self.token_analysis_data
                .token_success_rates
                .insert(token_str.clone(), new_rate);
        }

        // Update positive patterns
        for token_str in &token_strings {
            *self
                .token_analysis_data
                .positive_patterns
                .entry(token_str.clone())
                .or_insert(0) += 1;
        }

        // Update sequence scores
        let sequence = token_strings.join(" ");
        let current_score = self
            .token_analysis_data
            .sequence_scores
            .get(&sequence)
            .unwrap_or(&0.5);
        let new_score = (current_score * 0.8) + (1.0 * 0.2);
        self.token_analysis_data
            .sequence_scores
            .insert(sequence, new_score);

        // Update pattern combinations
        let pattern_key = format!("{}:{}", pattern_name, token_strings.join(","));
        let current_combo = self
            .token_analysis_data
            .pattern_combinations
            .get(&pattern_key)
            .unwrap_or(&0.5);
        let new_combo = (current_combo * 0.9) + (1.0 * 0.1);
        self.token_analysis_data
            .pattern_combinations
            .insert(pattern_key, new_combo);

        self.save_learning_data(context)?;
        Ok(())
    }
    /// Load learning data from the existing C2R cache system
    fn load_learning_data(&mut self, context: &mut Context) -> Result<()> {
        // Try to load machine learning data from the cache using Context
        let learning_data_id = Id::get(&"samplizer_learning_data");

        let entry: Entry = context.registry.get_entry(learning_data_id).clone();
        match entry {
            Entry::Str(json_string) => match crate::json::parsing::parse(&json_string) {
                Ok(json_value) => {
                    if let Err(e) = self.parse_learning_data_from_json(&json_value) {
                        report!(
                            context,
                            "samplizer",
                            "load_learning_data",
                            Info,
                            Process(Some("load_learning_data".to_string())),
                            format!("Failed to parse learning data from cache: {}", e),
                            false,
                            0,
                            0
                        );
                    } else {
                        report!(
                            context,
                            "samplizer",
                            "load_learning_data",
                            Info,
                            Process(Some("load_learning_data".to_string())),
                            "Successfully loaded learning data from cache",
                            true,
                            0,
                            0
                        );
                    }
                }
                Err(e) => {
                    report!(
                        context,
                        "samplizer",
                        "load_learning_data",
                        Info,
                        Process(Some("load_learning_data".to_string())),
                        format!("Failed to parse JSON: {}", e),
                        false,
                        0,
                        0
                    );
                }
            },
            _ => {
                report!(
                    context,
                    "samplizer",
                    "load_learning_data",
                    Info,
                    Process(Some("load_learning_data".to_string())),
                    "Learning data in cache is not string format",
                    false,
                    0,
                    0
                );
            }
        }
        Ok(())
    }

    /// Save learning data to the existing C2R cache system
    fn save_learning_data(&self, context: &mut Context) -> Result<()> {
        let json_data = self.create_learning_data_json();
        let learning_data_id = Id::get(&"samplizer_learning_data");

        // Store the learning data in the Context registry cache
        let json_string = crate::json::stringify(json_data);
        context
            .registry
            .insert(learning_data_id, Entry::Str(json_string));

        // Also trigger the pattern cache save to persist to disk
        // This will save all cache data including our learning data to ~/.c2r/cache
        let cache_dir = std::env::var("C2R_CACHE_DIR").unwrap_or_else(|_| {
            format!(
                "{}/.c2r/cache",
                std::env::var("HOME").unwrap_or_else(|_| "/tmp".to_string())
            )
        });

        let cache_file = Path::new(&cache_dir).join("pattern_cache.json");
        let cache_path = cache_file.to_string_lossy();

        // Use Context to save the entire cache including our learning data
        if let Err(e) = context.patternizer.save_cache_to_json(&cache_path) {
            report!(
                context,
                "samplizer",
                "save_learning_data",
                Info,
                Process(Some("save_learning_data".to_string())),
                format!("Failed to save learning data to cache: {}", e),
                false,
                0,
                0
            );
        } else {
            report!(
                context,
                "samplizer",
                "save_learning_data",
                Info,
                Process(Some("save_learning_data".to_string())),
                "Successfully saved learning data to cache",
                true,
                0,
                0
            );
        }

        Ok(())
    }

    /// Create JSON representation of learning data
    fn create_learning_data_json(&self) -> Value {
        let mut obj = Object::new();

        // Token success rates
        let mut token_rates = Object::new();
        for (token, rate) in &self.token_analysis_data.token_success_rates {
            token_rates.insert(token, Value::Number((*rate).into()));
        }
        obj.insert("token_success_rates", Value::Object(token_rates));

        // Pattern combinations
        let mut pattern_combos = Object::new();
        for (pattern, rate) in &self.token_analysis_data.pattern_combinations {
            pattern_combos.insert(pattern, Value::Number((*rate).into()));
        }
        obj.insert("pattern_combinations", Value::Object(pattern_combos));

        // Negative patterns
        let mut negative = Object::new();
        for (pattern, count) in &self.token_analysis_data.negative_patterns {
            negative.insert(pattern, Value::Number((*count as f64).into()));
        }
        obj.insert("negative_patterns", Value::Object(negative));

        // Positive patterns
        let mut positive = Object::new();
        for (pattern, count) in &self.token_analysis_data.positive_patterns {
            positive.insert(pattern, Value::Number((*count as f64).into()));
        }
        obj.insert("positive_patterns", Value::Object(positive));

        // Sequence scores
        let mut sequences = Object::new();
        for (seq, score) in &self.token_analysis_data.sequence_scores {
            sequences.insert(seq, Value::Number((*score).into()));
        }
        obj.insert("sequence_scores", Value::Object(sequences));

        Value::Object(obj)
    }

    /// Parse learning data from JSON
    fn parse_learning_data_from_json(&mut self, json: &Value) -> Result<()> {
        if let Value::Object(obj) = json {
            // Parse token success rates
            if let Some(Value::Object(token_rates)) = obj.get("token_success_rates") {
                for node in token_rates.iter() {
                    if let Value::Number(rate) = &node.value {
                        self.token_analysis_data
                            .token_success_rates
                            .insert(node.key.as_str().to_string(), f64::from(*rate));
                    }
                }
            }

            // Parse pattern combinations
            if let Some(Value::Object(pattern_combos)) = obj.get("pattern_combinations") {
                for node in pattern_combos.iter() {
                    if let Value::Number(rate) = &node.value {
                        self.token_analysis_data
                            .pattern_combinations
                            .insert(node.key.as_str().to_string(), f64::from(*rate));
                    }
                }
            }

            // Parse negative patterns
            if let Some(Value::Object(negative)) = obj.get("negative_patterns") {
                for node in negative.iter() {
                    if let Value::Number(count) = &node.value {
                        self.token_analysis_data
                            .negative_patterns
                            .insert(node.key.as_str().to_string(), f64::from(*count) as i32);
                    }
                }
            }

            // Parse positive patterns
            if let Some(Value::Object(positive)) = obj.get("positive_patterns") {
                for node in positive.iter() {
                    if let Value::Number(count) = &node.value {
                        self.token_analysis_data
                            .positive_patterns
                            .insert(node.key.as_str().to_string(), f64::from(*count) as i32);
                    }
                }
            }

            // Parse sequence scores
            if let Some(Value::Object(sequences)) = obj.get("sequence_scores") {
                for node in sequences.iter() {
                    if let Value::Number(score) = &node.value {
                        self.token_analysis_data
                            .sequence_scores
                            .insert(node.key.as_str().to_string(), f64::from(*score));
                    }
                }
            }
        }

        Ok(())
    }

    /// Analyze existing cache results to extract learning data
    fn analyze_existing_cache_results(&mut self, context: &mut Context) -> Result<()> {
        // Access the match cache to learn from existing positive/negative results

        for (cache_key, pattern_result) in context.patternizer.match_cache.clone() {
            // Extract tokens from cache key (cache keys often contain token info)
            let tokens = self.extract_tokens_from_cache_key(&cache_key);

            match pattern_result {
                PatternResult::Match { .. } => {
                    // This was a successful match - record as positive
                    for token in &tokens {
                        *self
                            .token_analysis_data
                            .positive_patterns
                            .entry(token.clone())
                            .or_insert(0) += 1;

                        // Update success rate
                        let current_rate = self
                            .token_analysis_data
                            .token_success_rates
                            .get(token)
                            .unwrap_or(&0.5);
                        let new_rate = (current_rate * 0.95) + (1.0 * 0.05); // Gentle learning from cache
                        self.token_analysis_data
                            .token_success_rates
                            .insert(token.clone(), new_rate);
                    }

                    // Update sequence scores
                    let sequence = tokens.join(" ");
                    let current_score = self
                        .token_analysis_data
                        .sequence_scores
                        .get(&sequence)
                        .unwrap_or(&0.5);
                    let new_score = (current_score * 0.95) + (1.0 * 0.05);
                    self.token_analysis_data
                        .sequence_scores
                        .insert(sequence, new_score);
                }
                PatternResult::NoMatch { .. } | PatternResult::Reject { .. } => {
                    // This was a failed match - record as negative
                    for token in &tokens {
                        *self
                            .token_analysis_data
                            .negative_patterns
                            .entry(token.clone())
                            .or_insert(0) += 1;

                        // Update success rate (decrease)
                        let current_rate = self
                            .token_analysis_data
                            .token_success_rates
                            .get(token)
                            .unwrap_or(&0.5);
                        let new_rate = (current_rate * 0.95) + (0.0 * 0.05); // Gentle learning from cache
                        self.token_analysis_data
                            .token_success_rates
                            .insert(token.clone(), new_rate);
                    }

                    // Update sequence scores (decrease)
                    let sequence = tokens.join(" ");
                    let current_score = self
                        .token_analysis_data
                        .sequence_scores
                        .get(&sequence)
                        .unwrap_or(&0.5);
                    let new_score = (current_score * 0.95) + (0.0 * 0.05);
                    self.token_analysis_data
                        .sequence_scores
                        .insert(sequence, new_score);
                }
                _ => {
                    // Other result types (Fuzzy, Sequence, etc.) - treat as partial positive
                    for token in &tokens {
                        let current_rate = self
                            .token_analysis_data
                            .token_success_rates
                            .get(token)
                            .unwrap_or(&0.5);
                        let new_rate = (current_rate * 0.98) + (0.7 * 0.02); // Partial success
                        self.token_analysis_data
                            .token_success_rates
                            .insert(token.clone(), new_rate);
                    }
                }
            }
        }
        Ok(())
    }

    /// Extract tokens from cache key format
    fn extract_tokens_from_cache_key(&self, cache_key: &str) -> Vec<String> {
        // Cache keys might be in formats like:
        // "pattern_name:token1,token2,token3"
        // "fingerprint_hash"
        // "function_declaration:int,main,("

        if let Some(colon_pos) = cache_key.find(':') {
            let token_part = &cache_key[colon_pos + 1..];
            token_part
                .split([',', ' ', ';'])
                .map(|s| s.trim().to_string())
                .filter(|s| !s.is_empty() && s.len() > 1) // Filter out single chars and empty
                .collect()
        } else {
            // Try to extract meaningful tokens from the key itself
            cache_key
                .split(['_', '-', ' '])
                .map(|s| s.trim().to_string())
                .filter(|s| !s.is_empty() && s.len() > 2) // Meaningful tokens only
                .collect()
        }
    }
}
#[derive(Debug, Clone, PartialEq)]
pub struct CaptureSystem {
    captures: HashMap<String, CapturedValue>,
    confidence_scores: HashMap<String, f64>,
    pub pattern_metadata: PatternMetadata,
}

impl Eq for CaptureSystem {}

impl std::hash::Hash for CaptureSystem {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.captures.iter().for_each(|(k, v)| {
            k.hash(state);
            v.hash(state);
        });
        self.confidence_scores.iter().for_each(|(k, v)| {
            k.hash(state);
            v.to_bits().hash(state);
        });
        self.pattern_metadata.hash(state);
    }
}

/// Represents different types of captured values from patterns
#[derive(Debug, Clone, PartialEq)]
pub enum CapturedValue {
    Text(String),
    TokenRange(Range<usize>),
    TokenSequence(Vec<Token>),
    Numeric(f64),
    Boolean(bool),
    List(Vec<CapturedValue>),
    Map(HashMap<String, CapturedValue>),
}

impl std::hash::Hash for CapturedValue {
    fn hash<H: Hasher>(&self, state: &mut H) {
        match self {
            CapturedValue::Text(s) => {
                0u8.hash(state);
                s.hash(state);
            }
            CapturedValue::TokenRange(range) => {
                1u8.hash(state);
                range.start.hash(state);
                range.end.hash(state);
            }
            CapturedValue::TokenSequence(tokens) => {
                2u8.hash(state);
                tokens.hash(state);
            }
            CapturedValue::Numeric(n) => {
                3u8.hash(state);
                n.to_bits().hash(state);
            }
            CapturedValue::Boolean(b) => {
                4u8.hash(state);
                b.hash(state);
            }
            CapturedValue::List(list) => {
                5u8.hash(state);
                list.hash(state);
            }
            CapturedValue::Map(map) => {
                6u8.hash(state);
                map.iter().for_each(|(k, v)| {
                    k.hash(state);
                    v.hash(state);
                });
            }
        }
    }
}

/// Metadata about the pattern matching process
#[derive(Debug, Clone, PartialEq, PartialOrd)]
pub struct PatternMetadata {
    pub pattern_name: String,
    pub match_confidence: f64,
    pub tokens_consumed: usize,
    pub match_type: PatternMatchType,
    pub timestamp: std::time::Instant,
}

impl Eq for PatternMetadata {}

impl std::hash::Hash for PatternMetadata {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.pattern_name.hash(state);
        self.match_confidence.to_bits().hash(state);
        self.tokens_consumed.hash(state);
        self.match_type.hash(state);
        self.timestamp.hash(state);
    }
}

/// Types of pattern matches
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub enum PatternMatchType {
    Exact,
    Fuzzy,
    Partial,
    Sequence,
    Structure,
}

/// Result of pattern analysis with captures
#[derive(Debug, Clone, PartialEq)]
pub struct AnalysisResult {
    pub captures: CaptureSystem,
    pub success: bool,
    pub confidence: f64,
    pub error_message: Option<String>,
}

impl Eq for AnalysisResult {}

impl std::hash::Hash for AnalysisResult {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.captures.hash(state);
        self.success.hash(state);
        self.confidence.to_bits().hash(state);
        self.error_message.hash(state);
    }
}

impl CaptureSystem {
    pub fn new(pattern_name: &str) -> Self {
        Self {
            captures: HashMap::new(),
            confidence_scores: HashMap::new(),
            pattern_metadata: PatternMetadata {
                pattern_name: pattern_name.to_string(),
                match_confidence: 0.0,
                tokens_consumed: 0,
                match_type: PatternMatchType::Exact,
                timestamp: std::time::Instant::now(),
            },
        }
    }

    /// Capture a text value
    pub fn capture_text(&mut self, key: &str, value: String, confidence: f64) {
        self.captures
            .insert(key.to_string(), CapturedValue::Text(value));
        self.confidence_scores.insert(key.to_string(), confidence);
    }

    /// Capture a token range
    pub fn capture_range(&mut self, key: &str, range: Range<usize>, confidence: f64) {
        self.captures
            .insert(key.to_string(), CapturedValue::TokenRange(range));
        self.confidence_scores.insert(key.to_string(), confidence);
    }

    /// Capture a sequence of tokens
    pub fn capture_tokens(&mut self, key: &str, tokens: Vec<Token>, confidence: f64) {
        self.captures
            .insert(key.to_string(), CapturedValue::TokenSequence(tokens));
        self.confidence_scores.insert(key.to_string(), confidence);
    }

    /// Capture a numeric value
    pub fn capture_numeric(&mut self, key: &str, value: f64, confidence: f64) {
        self.captures
            .insert(key.to_string(), CapturedValue::Numeric(value));
        self.confidence_scores.insert(key.to_string(), confidence);
    }

    /// Capture a boolean value
    pub fn capture_boolean(&mut self, key: &str, value: bool, confidence: f64) {
        self.captures
            .insert(key.to_string(), CapturedValue::Boolean(value));
        self.confidence_scores.insert(key.to_string(), confidence);
    }

    /// Capture a list of values
    pub fn capture_list(&mut self, key: &str, values: Vec<CapturedValue>, confidence: f64) {
        self.captures
            .insert(key.to_string(), CapturedValue::List(values));
        self.confidence_scores.insert(key.to_string(), confidence);
    }

    /// Get captured text value
    pub fn get_text(&self, key: &str) -> Option<&String> {
        match self.captures.get(key) {
            Some(CapturedValue::Text(text)) => Some(text),
            _ => None,
        }
    }

    /// Get captured token range
    pub fn get_range(&self, key: &str) -> Option<&Range<usize>> {
        match self.captures.get(key) {
            Some(CapturedValue::TokenRange(range)) => Some(range),
            _ => None,
        }
    }

    /// Get captured token sequence
    pub fn get_tokens(&self, key: &str) -> Option<&Vec<Token>> {
        match self.captures.get(key) {
            Some(CapturedValue::TokenSequence(tokens)) => Some(tokens),
            _ => None,
        }
    }

    /// Get captured numeric value
    pub fn get_numeric(&self, key: &str) -> Option<f64> {
        match self.captures.get(key) {
            Some(CapturedValue::Numeric(n)) => Some(*n),
            _ => None,
        }
    }

    /// Get captured boolean value
    pub fn get_boolean(&self, key: &str) -> Option<bool> {
        match self.captures.get(key) {
            Some(CapturedValue::Boolean(b)) => Some(*b),
            _ => None,
        }
    }

    /// Get captured list
    pub fn get_list(&self, key: &str) -> Option<&Vec<CapturedValue>> {
        match self.captures.get(key) {
            Some(CapturedValue::List(list)) => Some(list),
            _ => None,
        }
    }

    /// Check if a key exists in captures
    pub fn has_capture(&self, key: &str) -> bool {
        self.captures.contains_key(key)
    }

    /// Get confidence score for a capture
    pub fn get_confidence(&self, key: &str) -> Option<f64> {
        self.confidence_scores.get(key).copied()
    }

    /// Get all capture keys
    pub fn get_keys(&self) -> Vec<&String> {
        self.captures.keys().collect()
    }

    /// Get number of captures
    pub fn len(&self) -> usize {
        self.captures.len()
    }

    /// Check if captures are empty
    pub fn is_empty(&self) -> bool {
        self.captures.is_empty()
    }

    /// Set overall pattern metadata
    pub fn set_metadata(
        &mut self,
        confidence: f64,
        tokens_consumed: usize,
        match_type: PatternMatchType,
    ) {
        self.pattern_metadata.match_confidence = confidence;
        self.pattern_metadata.tokens_consumed = tokens_consumed;
        self.pattern_metadata.match_type = match_type;
    }

    /// Merge another capture system into this one
    pub fn merge(&mut self, other: CaptureSystem) {
        for (key, value) in other.captures {
            self.captures.insert(key.clone(), value);
            if let Some(confidence) = other.confidence_scores.get(&key) {
                self.confidence_scores.insert(key, *confidence);
            }
        }
    }

    /// Convert to HashMap for backward compatibility
    pub fn to_hashmap(&self) -> HashMap<String, String> {
        let mut result = HashMap::new();
        for (key, value) in &self.captures {
            let string_value = match value {
                CapturedValue::Text(s) => s.clone(),
                CapturedValue::TokenRange(range) => format!("{}..{}", range.start, range.end),
                CapturedValue::TokenSequence(tokens) => tokens
                    .iter()
                    .map(|t| t.to_string())
                    .collect::<Vec<_>>()
                    .join(" "),
                CapturedValue::Numeric(n) => n.to_string(),
                CapturedValue::Boolean(b) => b.to_string(),
                CapturedValue::List(list) => format!("[{}]", list.len()),
                CapturedValue::Map(map) => format!("{{{}}}", map.len()),
            };
            result.insert(key.clone(), string_value);
        }
        result
    }
}

impl AnalysisResult {
    pub fn success(captures: CaptureSystem, confidence: f64) -> Self {
        Self {
            captures,
            success: true,
            confidence,
            error_message: None,
        }
    }

    pub fn failure(error: String) -> Self {
        Self {
            captures: CaptureSystem::new("failed_pattern"),
            success: false,
            confidence: 0.0,
            error_message: Some(error),
        }
    }

    pub fn partial(captures: CaptureSystem, confidence: f64, warning: String) -> Self {
        Self {
            captures,
            success: true,
            confidence,
            error_message: Some(warning),
        }
    }
}
/// Create a new Samplizer instance
pub fn create_samplizer() -> Samplizer {
    Samplizer::new()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_samplizer_creation() {
        let samplizer = create_samplizer();
        assert_eq!(samplizer.generated_patterns.len(), 0);
        assert_eq!(samplizer.analyzed_pairs.len(), 0);
    }

    #[test]
    fn test_segment_type_classification() {
        let samplizer = create_samplizer();

        let struct_tokens = vec![
            Token::s("struct".to_string()),
            Token::s("Point".to_string()),
        ];

        assert_eq!(
            Samplizer::classify_segment_type(&struct_tokens),
            SegmentType::Struct
        );
    }
}
